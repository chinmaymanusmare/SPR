{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Complete GMM-based Bayes Classifier Implementation with Model Saving\n",
    "===================================================================\n",
    "\n",
    "This implementation provides all components needed for the assignment including:\n",
    "- K-means clustering\n",
    "- Gaussian Mixture Models  \n",
    "- GMM-based Bayes Classifier\n",
    "- Feature extraction for images\n",
    "- Comprehensive evaluation metrics\n",
    "- Visualization functions\n",
    "- Experimental pipeline\n",
    "- Model saving and loading functionality\n",
    "\n",
    "No external ML libraries used for core algorithms.\n",
    "Only numpy and matplotlib are used.\n",
    "\n",
    "Author: Assignment Implementation\n",
    "Date: 2025\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    \"\"\"Calculate Euclidean distance between two points\"\"\"\n",
    "    return np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "\n",
    "def multivariate_gaussian_pdf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Calculate multivariate Gaussian probability density function\n",
    "    x: data point (d,) or (n, d)\n",
    "    mu: mean vector (d,)\n",
    "    sigma: covariance matrix (d, d)\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(1, -1)\n",
    "    if mu.ndim == 1:\n",
    "        mu = mu.reshape(1, -1)\n",
    "\n",
    "    d = x.shape[1]  # dimensionality\n",
    "    n = x.shape[0]  # number of points\n",
    "\n",
    "    # Compute determinant and inverse of covariance matrix\n",
    "    det_sigma = np.linalg.det(sigma)\n",
    "    if det_sigma <= 0:\n",
    "        det_sigma = 1e-10  # Avoid division by zero\n",
    "\n",
    "    inv_sigma = np.linalg.pinv(sigma)  # Use pseudo-inverse for numerical stability\n",
    "\n",
    "    # Normalization constant\n",
    "    norm_const = 1.0 / np.sqrt((2 * np.pi) ** d * det_sigma)\n",
    "\n",
    "    # Calculate PDF for each point\n",
    "    pdf_values = []\n",
    "    for i in range(n):\n",
    "        x_centered = x[i:i+1] - mu\n",
    "        exponent = -0.5 * np.dot(np.dot(x_centered, inv_sigma), x_centered.T)[0, 0]\n",
    "        pdf_val = norm_const * np.exp(exponent)\n",
    "        pdf_values.append(pdf_val)\n",
    "\n",
    "    return np.array(pdf_values)\n",
    "\n",
    "# ============================================================================\n",
    "# K-MEANS CLUSTERING\n",
    "# ============================================================================\n",
    "\n",
    "class KMeans:\n",
    "    def __init__(self, k, max_iters=100, random_state=None):\n",
    "        self.k = k\n",
    "        self.max_iters = max_iters\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X):\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize centroids randomly\n",
    "        self.centroids = np.random.uniform(\n",
    "            low=X.min(axis=0), \n",
    "            high=X.max(axis=0), \n",
    "            size=(self.k, n_features)\n",
    "        )\n",
    "\n",
    "        self.labels = np.zeros(n_samples)\n",
    "\n",
    "        for iteration in range(self.max_iters):\n",
    "            # Assign points to closest centroid\n",
    "            distances = np.zeros((n_samples, self.k))\n",
    "\n",
    "            for i, centroid in enumerate(self.centroids):\n",
    "                distances[:, i] = np.sqrt(np.sum((X - centroid) ** 2, axis=1))\n",
    "\n",
    "            new_labels = np.argmin(distances, axis=1)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.array_equal(new_labels, self.labels):\n",
    "                print(f\"K-means converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "\n",
    "            self.labels = new_labels\n",
    "\n",
    "            # Update centroids\n",
    "            for i in range(self.k):\n",
    "                if np.sum(self.labels == i) > 0:\n",
    "                    self.centroids[i] = X[self.labels == i].mean(axis=0)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        distances = np.zeros((X.shape[0], self.k))\n",
    "        for i, centroid in enumerate(self.centroids):\n",
    "            distances[:, i] = np.sqrt(np.sum((X - centroid) ** 2, axis=1))\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        self.fit(X)\n",
    "        return self.labels\n",
    "\n",
    "# ============================================================================\n",
    "# GAUSSIAN MIXTURE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "class GaussianMixtureModel:\n",
    "    def __init__(self, n_components, max_iters=100, tol=1e-6, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        \"\"\"Initialize GMM parameters using K-means\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Use K-means for initialization\n",
    "        kmeans = KMeans(k=self.n_components, random_state=self.random_state)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "\n",
    "        # Initialize means from K-means centroids\n",
    "        self.means = kmeans.centroids.copy()\n",
    "\n",
    "        # Initialize covariances and weights\n",
    "        self.covariances = []\n",
    "        self.weights = np.zeros(self.n_components)\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            # Points assigned to cluster k\n",
    "            cluster_points = X[labels == k]\n",
    "\n",
    "            if len(cluster_points) > 0:\n",
    "                # Weight is proportion of points in cluster\n",
    "                self.weights[k] = len(cluster_points) / n_samples\n",
    "\n",
    "                # Covariance matrix for cluster k\n",
    "                if len(cluster_points) > 1:\n",
    "                    cov = np.cov(cluster_points.T)\n",
    "                    # Add small diagonal term for numerical stability\n",
    "                    if cov.ndim == 0:  # scalar case\n",
    "                        cov = np.array([[cov + 1e-6]])\n",
    "                    else:\n",
    "                        cov += np.eye(n_features) * 1e-6\n",
    "                else:\n",
    "                    cov = np.eye(n_features)\n",
    "\n",
    "                self.covariances.append(cov)\n",
    "            else:\n",
    "                # Empty cluster\n",
    "                self.weights[k] = 1.0 / self.n_components\n",
    "                self.covariances.append(np.eye(n_features))\n",
    "\n",
    "        # Normalize weights\n",
    "        self.weights = self.weights / np.sum(self.weights)\n",
    "\n",
    "    def _e_step(self, X):\n",
    "        \"\"\"Expectation step: compute responsibilities\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.n_components))\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            # Calculate likelihood for component k\n",
    "            likelihood = multivariate_gaussian_pdf(X, self.means[k], self.covariances[k])\n",
    "            responsibilities[:, k] = self.weights[k] * likelihood\n",
    "\n",
    "        # Normalize responsibilities\n",
    "        total_responsibility = np.sum(responsibilities, axis=1, keepdims=True)\n",
    "        total_responsibility[total_responsibility == 0] = 1e-10  # Avoid division by zero\n",
    "        responsibilities = responsibilities / total_responsibility\n",
    "\n",
    "        return responsibilities\n",
    "\n",
    "    def _m_step(self, X, responsibilities):\n",
    "        \"\"\"Maximization step: update parameters\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Update weights\n",
    "        self.weights = np.mean(responsibilities, axis=0)\n",
    "\n",
    "        # Update means and covariances\n",
    "        for k in range(self.n_components):\n",
    "            # Effective number of points assigned to component k\n",
    "            Nk = np.sum(responsibilities[:, k])\n",
    "\n",
    "            if Nk > 1e-10:  # Avoid division by zero\n",
    "                # Update mean\n",
    "                self.means[k] = np.sum(responsibilities[:, k:k+1] * X, axis=0) / Nk\n",
    "\n",
    "                # Update covariance\n",
    "                diff = X - self.means[k]\n",
    "                weighted_diff = responsibilities[:, k:k+1] * diff\n",
    "                cov = np.dot(weighted_diff.T, diff) / Nk\n",
    "\n",
    "                # Add regularization for numerical stability\n",
    "                cov += np.eye(n_features) * 1e-6\n",
    "                self.covariances[k] = cov\n",
    "\n",
    "    def _compute_log_likelihood(self, X):\n",
    "        \"\"\"Compute log-likelihood of the data\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        log_likelihood = 0\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sample_likelihood = 0\n",
    "            for k in range(self.n_components):\n",
    "                component_likelihood = self.weights[k] * multivariate_gaussian_pdf(\n",
    "                    X[i:i+1], self.means[k], self.covariances[k]\n",
    "                )[0]\n",
    "                sample_likelihood += component_likelihood\n",
    "\n",
    "            if sample_likelihood > 0:\n",
    "                log_likelihood += np.log(sample_likelihood)\n",
    "            else:\n",
    "                log_likelihood += -np.inf\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit GMM using EM algorithm\"\"\"\n",
    "        # Initialize parameters\n",
    "        self._initialize_parameters(X)\n",
    "\n",
    "        self.log_likelihoods = []\n",
    "\n",
    "        for iteration in range(self.max_iters):\n",
    "            # E-step\n",
    "            responsibilities = self._e_step(X)\n",
    "\n",
    "            # M-step\n",
    "            self._m_step(X, responsibilities)\n",
    "\n",
    "            # Compute log-likelihood\n",
    "            log_likelihood = self._compute_log_likelihood(X)\n",
    "            self.log_likelihoods.append(log_likelihood)\n",
    "\n",
    "            # Check convergence\n",
    "            if iteration > 0:\n",
    "                if abs(self.log_likelihoods[-1] - self.log_likelihoods[-2]) < self.tol:\n",
    "                    print(f\"GMM converged after {iteration + 1} iterations\")\n",
    "                    break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        return self._e_step(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster assignments\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# ============================================================================\n",
    "# GMM-BASED BAYES CLASSIFIER\n",
    "# ============================================================================\n",
    "\n",
    "class GMMBayesClassifier:\n",
    "    def __init__(self, n_components_per_class=2, max_iters=100, tol=1e-6, random_state=None):\n",
    "        self.n_components_per_class = n_components_per_class\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.gmm_models = {}\n",
    "        self.class_priors = {}\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train GMM-based Bayes classifier\"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_total_samples = len(y)\n",
    "\n",
    "        # Train separate GMM for each class\n",
    "        for class_label in self.classes:\n",
    "            # Get data for this class\n",
    "            class_data = X[y == class_label]\n",
    "\n",
    "            # Calculate class prior\n",
    "            self.class_priors[class_label] = len(class_data) / n_total_samples\n",
    "\n",
    "            # Train GMM for this class\n",
    "            if isinstance(self.n_components_per_class, dict):\n",
    "                n_components = self.n_components_per_class.get(class_label, 2)\n",
    "            else:\n",
    "                n_components = self.n_components_per_class\n",
    "\n",
    "            gmm = GaussianMixtureModel(\n",
    "                n_components=n_components,\n",
    "                max_iters=self.max_iters,\n",
    "                tol=self.tol,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "\n",
    "            print(f\"Training GMM for class {class_label} with {len(class_data)} samples...\")\n",
    "            gmm.fit(class_data)\n",
    "            self.gmm_models[class_label] = gmm\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _compute_class_likelihood(self, X, class_label):\n",
    "        \"\"\"Compute likelihood P(X|class)\"\"\"\n",
    "        gmm = self.gmm_models[class_label]\n",
    "        n_samples = X.shape[0]\n",
    "        likelihoods = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            sample_likelihood = 0\n",
    "            for k in range(gmm.n_components):\n",
    "                component_likelihood = gmm.weights[k] * multivariate_gaussian_pdf(\n",
    "                    X[i:i+1], gmm.means[k], gmm.covariances[k]\n",
    "                )[0]\n",
    "                sample_likelihood += component_likelihood\n",
    "            likelihoods[i] = sample_likelihood\n",
    "\n",
    "        return likelihoods\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities using Bayes rule\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        n_classes = len(self.classes)\n",
    "        probabilities = np.zeros((n_samples, n_classes))\n",
    "\n",
    "        # Compute posterior probabilities for each class\n",
    "        for i, class_label in enumerate(self.classes):\n",
    "            # P(X|class) * P(class)\n",
    "            class_likelihood = self._compute_class_likelihood(X, class_label)\n",
    "            probabilities[:, i] = class_likelihood * self.class_priors[class_label]\n",
    "\n",
    "        # Normalize to get P(class|X)\n",
    "        total_prob = np.sum(probabilities, axis=1, keepdims=True)\n",
    "        total_prob[total_prob == 0] = 1e-10  # Avoid division by zero\n",
    "        probabilities = probabilities / total_prob\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        predicted_indices = np.argmax(probabilities, axis=1)\n",
    "        return self.classes[predicted_indices]\n",
    "\n",
    "    def get_log_likelihood(self, X, y):\n",
    "        \"\"\"Get log-likelihood for model selection\"\"\"\n",
    "        total_log_likelihood = 0\n",
    "        for class_label in self.classes:\n",
    "            class_data = X[y == class_label]\n",
    "            if len(class_data) > 0:\n",
    "                class_log_likelihood = self.gmm_models[class_label]._compute_log_likelihood(class_data)\n",
    "                total_log_likelihood += class_log_likelihood\n",
    "        return total_log_likelihood\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE EXTRACTION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_color_histogram_features(image, patch_size=32, n_bins=8):\n",
    "    \"\"\"\n",
    "    Extract color histogram features from image patches\n",
    "\n",
    "    Parameters:\n",
    "    - image: numpy array of shape (H, W, 3) - RGB image\n",
    "    - patch_size: size of non-overlapping patches\n",
    "    - n_bins: number of histogram bins per channel\n",
    "\n",
    "    Returns:\n",
    "    - features: list of 24-dimensional feature vectors (8 bins Ã— 3 channels)\n",
    "    \"\"\"\n",
    "    if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "        raise ValueError(\"Image must be RGB with shape (H, W, 3)\")\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "    features = []\n",
    "\n",
    "    # Calculate number of patches\n",
    "    n_patches_h = height // patch_size\n",
    "    n_patches_w = width // patch_size\n",
    "\n",
    "    # Extract patches and compute histograms\n",
    "    for i in range(n_patches_h):\n",
    "        for j in range(n_patches_w):\n",
    "            # Extract patch\n",
    "            start_h = i * patch_size\n",
    "            end_h = start_h + patch_size\n",
    "            start_w = j * patch_size\n",
    "            end_w = start_w + patch_size\n",
    "\n",
    "            patch = image[start_h:end_h, start_w:end_w, :]\n",
    "\n",
    "            # Compute histogram for each channel\n",
    "            feature_vector = []\n",
    "            for c in range(3):  # RGB channels\n",
    "                channel_data = patch[:, :, c].flatten()\n",
    "\n",
    "                # Create histogram with n_bins\n",
    "                hist, _ = np.histogram(channel_data, bins=n_bins, range=(0, 255))\n",
    "\n",
    "                # Normalize by number of pixels in patch\n",
    "                hist_normalized = hist / (patch_size * patch_size)\n",
    "                feature_vector.extend(hist_normalized)\n",
    "\n",
    "            features.append(np.array(feature_vector))\n",
    "\n",
    "    return features\n",
    "\n",
    "def build_visual_vocabulary(all_features, vocab_size=32, random_state=None):\n",
    "    \"\"\"\n",
    "    Build visual vocabulary using K-means clustering\n",
    "\n",
    "    Parameters:\n",
    "    - all_features: list of feature vectors from all training images\n",
    "    - vocab_size: number of visual words in vocabulary\n",
    "\n",
    "    Returns:\n",
    "    - vocabulary: K-means model representing visual words\n",
    "    \"\"\"\n",
    "    # Combine all features into single array\n",
    "    features_array = np.vstack(all_features)\n",
    "\n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(k=vocab_size, random_state=random_state)\n",
    "    kmeans.fit(features_array)\n",
    "\n",
    "    return kmeans\n",
    "\n",
    "def extract_bag_of_visual_words(image_features, vocabulary):\n",
    "    \"\"\"\n",
    "    Extract bag-of-visual-words representation for an image\n",
    "\n",
    "    Parameters:\n",
    "    - image_features: list of feature vectors from image patches\n",
    "    - vocabulary: trained K-means model (visual vocabulary)\n",
    "\n",
    "    Returns:\n",
    "    - bovw_vector: normalized histogram of visual word occurrences\n",
    "    \"\"\"\n",
    "    if len(image_features) == 0:\n",
    "        return np.zeros(vocabulary.k)\n",
    "\n",
    "    # Convert to array\n",
    "    features_array = np.vstack(image_features)\n",
    "\n",
    "    # Assign each feature to nearest visual word\n",
    "    assignments = vocabulary.predict(features_array)\n",
    "\n",
    "    # Create histogram of assignments\n",
    "    bovw_vector = np.zeros(vocabulary.k)\n",
    "    for assignment in assignments:\n",
    "        bovw_vector[assignment] += 1\n",
    "\n",
    "    # Normalize by total number of features\n",
    "    bovw_vector = bovw_vector / len(image_features)\n",
    "\n",
    "    return bovw_vector\n",
    "\n",
    "def extract_cell_image_features(image, patch_size=7, stride=1):\n",
    "    \"\"\"\n",
    "    Extract features from cell images using overlapping patches\n",
    "\n",
    "    Parameters:\n",
    "    - image: grayscale image array (H, W)\n",
    "    - patch_size: size of patches (7x7)\n",
    "    - stride: step size for patch extraction\n",
    "\n",
    "    Returns:\n",
    "    - features: list of 2-dimensional feature vectors (mean, std)\n",
    "    \"\"\"\n",
    "    if len(image.shape) != 2:\n",
    "        raise ValueError(\"Image must be grayscale with shape (H, W)\")\n",
    "\n",
    "    height, width = image.shape\n",
    "    features = []\n",
    "\n",
    "    # Extract overlapping patches\n",
    "    for i in range(0, height - patch_size + 1, stride):\n",
    "        for j in range(0, width - patch_size + 1, stride):\n",
    "            # Extract patch\n",
    "            patch = image[i:i+patch_size, j:j+patch_size]\n",
    "\n",
    "            # Compute mean and standard deviation\n",
    "            patch_mean = np.mean(patch)\n",
    "            patch_std = np.std(patch)\n",
    "\n",
    "            features.append(np.array([patch_mean, patch_std]))\n",
    "\n",
    "    return features\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def confusion_matrix(y_true, y_pred, classes=None):\n",
    "    \"\"\"Compute confusion matrix\"\"\"\n",
    "    if classes is None:\n",
    "        classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "\n",
    "    n_classes = len(classes)\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    # Create mapping from class labels to indices\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        true_idx = class_to_idx[true_label]\n",
    "        pred_idx = class_to_idx[pred_label]\n",
    "        cm[true_idx, pred_idx] += 1\n",
    "\n",
    "    return cm, classes\n",
    "\n",
    "def classification_metrics(y_true, y_pred, classes=None):\n",
    "    \"\"\"Compute comprehensive classification metrics\"\"\"\n",
    "    cm, class_labels = confusion_matrix(y_true, y_pred, classes)\n",
    "    n_classes = len(class_labels)\n",
    "\n",
    "    # Initialize metrics\n",
    "    precision = np.zeros(n_classes)\n",
    "    recall = np.zeros(n_classes)\n",
    "    f1_score = np.zeros(n_classes)\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    for i in range(n_classes):\n",
    "        # True positives, false positives, false negatives\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "\n",
    "        # Precision = TP / (TP + FP)\n",
    "        if tp + fp > 0:\n",
    "            precision[i] = tp / (tp + fp)\n",
    "        else:\n",
    "            precision[i] = 0.0\n",
    "\n",
    "        # Recall = TP / (TP + FN)  \n",
    "        if tp + fn > 0:\n",
    "            recall[i] = tp / (tp + fn)\n",
    "        else:\n",
    "            recall[i] = 0.0\n",
    "\n",
    "        # F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "        if precision[i] + recall[i] > 0:\n",
    "            f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "        else:\n",
    "            f1_score[i] = 0.0\n",
    "\n",
    "    # Overall accuracy\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "    # Mean metrics\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)  \n",
    "    mean_f1 = np.mean(f1_score)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'mean_precision': mean_precision,\n",
    "        'mean_recall': mean_recall,\n",
    "        'mean_f1': mean_f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_labels': class_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def print_classification_report(metrics):\n",
    "    \"\"\"Print detailed classification report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(f\"\\nOverall Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Mean Precision:   {metrics['mean_precision']:.4f}\")\n",
    "    print(f\"Mean Recall:      {metrics['mean_recall']:.4f}\")\n",
    "    print(f\"Mean F1-Score:    {metrics['mean_f1']:.4f}\")\n",
    "\n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    print(f\"{'Class':<8} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}\")\n",
    "    print(\"-\" * 48)\n",
    "\n",
    "    for i, class_label in enumerate(metrics['class_labels']):\n",
    "        print(f\"{class_label:<8} {metrics['precision'][i]:<12.4f} \"\n",
    "              f\"{metrics['recall'][i]:<12.4f} {metrics['f1_score'][i]:<12.4f}\")\n",
    "\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(\"Rows: True labels, Columns: Predicted labels\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION FUNCTIONS  \n",
    "# ============================================================================\n",
    "\n",
    "def plot_decision_boundary(X, y, classifier, resolution=100, title=\"Decision Boundary\"):\n",
    "    \"\"\"Plot decision boundary for 2D data\"\"\"\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Decision boundary plot only available for 2D data\")\n",
    "        return\n",
    "\n",
    "    # Create a mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "\n",
    "    # Make predictions on mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = classifier.predict(mesh_points)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    # Plot data points\n",
    "    unique_classes = np.unique(y)\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "    for i, class_label in enumerate(unique_classes):\n",
    "        class_data = X[y == class_label]\n",
    "        plt.scatter(class_data[:, 0], class_data[:, 1], \n",
    "                   c=colors[i % len(colors)], label=f'Class {class_label}',\n",
    "                   edgecolors='black', s=50)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_contour_gmm(X, y, classifier, resolution=100, title=\"GMM Contour Plot\"):\n",
    "    \"\"\"Plot probability contours for GMM\"\"\"\n",
    "    if X.shape[1] != 2:\n",
    "        print(\"Contour plot only available for 2D data\")\n",
    "        return\n",
    "\n",
    "    # Create a mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "\n",
    "    # Make probability predictions on mesh\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z_proba = classifier.predict_proba(mesh_points)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot contours for each class\n",
    "    unique_classes = np.unique(y)\n",
    "    colors = ['Reds', 'Blues', 'Greens', 'Oranges', 'Purples']\n",
    "\n",
    "    for i, class_label in enumerate(unique_classes):\n",
    "        class_idx = np.where(classifier.classes == class_label)[0][0]\n",
    "        Z_class = Z_proba[:, class_idx].reshape(xx.shape)\n",
    "\n",
    "        plt.contour(xx, yy, Z_class, levels=5, colors=colors[i % len(colors)], \n",
    "                   alpha=0.6, linewidths=2)\n",
    "        plt.contourf(xx, yy, Z_class, levels=5, cmap=colors[i % len(colors)], \n",
    "                    alpha=0.2)\n",
    "\n",
    "    # Plot data points\n",
    "    colors_scatter = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, class_label in enumerate(unique_classes):\n",
    "        class_data = X[y == class_label]\n",
    "        plt.scatter(class_data[:, 0], class_data[:, 1], \n",
    "                   c=colors_scatter[i % len(colors_scatter)], \n",
    "                   label=f'Class {class_label}',\n",
    "                   edgecolors='black', s=50)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_log_likelihood_convergence(log_likelihoods, title=\"Log-Likelihood Convergence\"):\n",
    "    \"\"\"Plot log-likelihood vs iterations\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(log_likelihoods, 'b-', linewidth=2, marker='o')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Log-Likelihood')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    return plt.gcf()\n",
    "\n",
    "def plot_segmentation_results(original_image, segmented_image, title=\"Segmentation Results\"):\n",
    "    \"\"\"Plot original and segmented images side by side\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if len(original_image.shape) == 3:\n",
    "        plt.imshow(original_image)\n",
    "    else:\n",
    "        plt.imshow(original_image, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(segmented_image, cmap='viridis')\n",
    "    plt.title('Segmented Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    return plt.gcf()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL SAVING AND LOADING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def create_model_save_directory(base_dir=\"saved_models\"):\n",
    "    \"\"\"Create directory structure with current date and time\"\"\"\n",
    "    current_time = datetime.now()\n",
    "    timestamp = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    base_path = os.path.join(base_dir, timestamp)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    print(f\"Created base directory: {base_path}\")\n",
    "    return base_path\n",
    "\n",
    "def save_gmm_model(classifier, n_components, base_path, X_train, y_train, X_test, y_test, \n",
    "                   y_pred, metrics, log_likelihood, additional_data=None):\n",
    "    \"\"\"\n",
    "    Save complete GMM model parameters and results\n",
    "\n",
    "    Parameters:\n",
    "    - classifier: trained GMMBayesClassifier\n",
    "    - n_components: number of components used\n",
    "    - base_path: base directory path\n",
    "    - X_train, y_train: training data\n",
    "    - X_test, y_test: test data  \n",
    "    - y_pred: predictions\n",
    "    - metrics: evaluation metrics dictionary\n",
    "    - log_likelihood,\n",
    "    - additional_data: any additional data to save\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subdirectory for this number of components\n",
    "    model_dir = os.path.join(base_path, f\"k_{n_components}\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving model with {n_components} components to: {model_dir}\")\n",
    "\n",
    "    # Save training and test data\n",
    "    np.save(os.path.join(model_dir, \"X_train.npy\"), X_train)\n",
    "    np.save(os.path.join(model_dir, \"y_train.npy\"), y_train)\n",
    "    np.save(os.path.join(model_dir, \"X_test.npy\"), X_test)\n",
    "    np.save(os.path.join(model_dir, \"y_test.npy\"), y_test)\n",
    "    np.save(os.path.join(model_dir, \"y_pred.npy\"), y_pred)\n",
    "\n",
    "    # Save model parameters for each class\n",
    "    for class_label in classifier.classes:\n",
    "        class_dir = os.path.join(model_dir, f\"class_{class_label}\")\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "        gmm = classifier.gmm_models[class_label]\n",
    "\n",
    "        # Save GMM parameters\n",
    "        np.save(os.path.join(class_dir, \"means.npy\"), np.array(gmm.means))\n",
    "        np.save(os.path.join(class_dir, \"covariances.npy\"), np.array(gmm.covariances))\n",
    "        np.save(os.path.join(class_dir, \"weights.npy\"), gmm.weights)\n",
    "        np.save(os.path.join(class_dir, \"log_likelihoods.npy\"), np.array(gmm.log_likelihoods))\n",
    "\n",
    "        # Save convergence information\n",
    "        convergence_info = {\n",
    "            'n_components': gmm.n_components,\n",
    "            'max_iters': gmm.max_iters,\n",
    "            'tol': gmm.tol,\n",
    "            'final_log_likelihood': gmm.log_likelihoods[-1] if gmm.log_likelihoods else None,\n",
    "            'n_iterations': len(gmm.log_likelihoods)\n",
    "        }\n",
    "        np.save(os.path.join(class_dir, \"convergence_info.npy\"), convergence_info)\n",
    "\n",
    "    # Save classifier-level parameters\n",
    "    classifier_params = {\n",
    "        'classes': classifier.classes,\n",
    "        'class_priors': classifier.class_priors,\n",
    "        'n_components_per_class': classifier.n_components_per_class\n",
    "    }\n",
    "    np.save(os.path.join(model_dir, \"classifier_params.npy\"), classifier_params)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    np.save(os.path.join(model_dir, \"confusion_matrix.npy\"), metrics['confusion_matrix'])\n",
    "    np.save(os.path.join(model_dir, \"precision.npy\"), metrics['precision'])\n",
    "    np.save(os.path.join(model_dir, \"recall.npy\"), metrics['recall'])\n",
    "    np.save(os.path.join(model_dir, \"f1_score.npy\"), metrics['f1_score'])\n",
    "\n",
    "    # Save aggregate metrics\n",
    "    aggregate_metrics = {\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'mean_precision': metrics['mean_precision'],\n",
    "        'mean_recall': metrics['mean_recall'],\n",
    "        'mean_f1': metrics['mean_f1'],\n",
    "        'class_labels': metrics['class_labels']\n",
    "    }\n",
    "    np.save(os.path.join(model_dir, \"aggregate_metrics.npy\"), aggregate_metrics)\n",
    "\n",
    "    # Save model selection scores\n",
    "    model_scores = {\n",
    "        'log_likelihood': log_likelihood,\n",
    "        'n_components': n_components,\n",
    "        'n_features': X_train.shape[1],\n",
    "        'n_classes': len(classifier.classes),\n",
    "        'n_train_samples': len(X_train),\n",
    "        'n_test_samples': len(X_test)\n",
    "    }\n",
    "    np.save(os.path.join(model_dir, \"model_scores.npy\"), model_scores)\n",
    "\n",
    "    # Save additional data if provided\n",
    "    if additional_data:\n",
    "        np.save(os.path.join(model_dir, \"additional_data.npy\"), additional_data)\n",
    "\n",
    "    # Create a summary text file\n",
    "    summary_path = os.path.join(model_dir, \"model_summary.txt\")\n",
    "    with open(summary_path, 'w') as f:\n",
    "        f.write(f\"GMM-Based Bayes Classifier Model Summary\\n\")\n",
    "        f.write(f\"========================================\\n\\n\")\n",
    "        f.write(f\"Number of Components: {n_components}\\n\")\n",
    "        f.write(f\"Number of Classes: {len(classifier.classes)}\\n\")\n",
    "        f.write(f\"Classes: {list(classifier.classes)}\\n\")\n",
    "        f.write(f\"Feature Dimensions: {X_train.shape[1]}\\n\")\n",
    "        f.write(f\"Training Samples: {len(X_train)}\\n\")\n",
    "        f.write(f\"Test Samples: {len(X_test)}\\n\\n\")\n",
    "\n",
    "        f.write(f\"Performance Metrics:\\n\")\n",
    "        f.write(f\"- Accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "        f.write(f\"- Mean Precision: {metrics['mean_precision']:.4f}\\n\")\n",
    "        f.write(f\"- Mean Recall: {metrics['mean_recall']:.4f}\\n\")\n",
    "        f.write(f\"- Mean F1-Score: {metrics['mean_f1']:.4f}\\n\\n\")\n",
    "\n",
    "        f.write(f\"Model Selection Scores:\\n\")\n",
    "        f.write(f\"- Log-Likelihood: {log_likelihood:.4f}\\n\")\n",
    "        f.write(f\"Per-Class Performance:\\n\")\n",
    "        for i, class_label in enumerate(metrics['class_labels']):\n",
    "            f.write(f\"- Class {class_label}: Precision={metrics['precision'][i]:.4f}, \"\n",
    "                   f\"Recall={metrics['recall'][i]:.4f}, F1={metrics['f1_score'][i]:.4f}\\n\")\n",
    "\n",
    "    print(f\"Model saved successfully to {model_dir}\")\n",
    "    return model_dir\n",
    "\n",
    "def load_gmm_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a saved GMM model and all its parameters\n",
    "\n",
    "    Parameters:\n",
    "    - model_dir: directory containing saved model\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary containing all loaded model data\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Loading model from: {model_dir}\")\n",
    "\n",
    "    # Load basic data\n",
    "    model_data = {}\n",
    "    model_data['X_train'] = np.load(os.path.join(model_dir, \"X_train.npy\"))\n",
    "    model_data['y_train'] = np.load(os.path.join(model_dir, \"y_train.npy\"))\n",
    "    model_data['X_test'] = np.load(os.path.join(model_dir, \"X_test.npy\"))\n",
    "    model_data['y_test'] = np.load(os.path.join(model_dir, \"y_test.npy\"))\n",
    "    model_data['y_pred'] = np.load(os.path.join(model_dir, \"y_pred.npy\"))\n",
    "\n",
    "    # Load classifier parameters\n",
    "    classifier_params = np.load(os.path.join(model_dir, \"classifier_params.npy\"), allow_pickle=True).item()\n",
    "    model_data['classifier_params'] = classifier_params\n",
    "\n",
    "    # Load class-specific GMM parameters\n",
    "    model_data['gmm_params'] = {}\n",
    "    for class_label in classifier_params['classes']:\n",
    "        class_dir = os.path.join(model_dir, f\"class_{class_label}\")\n",
    "\n",
    "        class_params = {}\n",
    "        class_params['means'] = np.load(os.path.join(class_dir, \"means.npy\"))\n",
    "        class_params['covariances'] = np.load(os.path.join(class_dir, \"covariances.npy\"))\n",
    "        class_params['weights'] = np.load(os.path.join(class_dir, \"weights.npy\"))\n",
    "        class_params['log_likelihoods'] = np.load(os.path.join(class_dir, \"log_likelihoods.npy\"))\n",
    "        class_params['convergence_info'] = np.load(os.path.join(class_dir, \"convergence_info.npy\"), allow_pickle=True).item()\n",
    "\n",
    "        model_data['gmm_params'][class_label] = class_params\n",
    "\n",
    "    # Load evaluation metrics\n",
    "    model_data['confusion_matrix'] = np.load(os.path.join(model_dir, \"confusion_matrix.npy\"))\n",
    "    model_data['precision'] = np.load(os.path.join(model_dir, \"precision.npy\"))\n",
    "    model_data['recall'] = np.load(os.path.join(model_dir, \"recall.npy\"))\n",
    "    model_data['f1_score'] = np.load(os.path.join(model_dir, \"f1_score.npy\"))\n",
    "    model_data['aggregate_metrics'] = np.load(os.path.join(model_dir, \"aggregate_metrics.npy\"), allow_pickle=True).item()\n",
    "\n",
    "    # Load model scores\n",
    "    model_data['model_scores'] = np.load(os.path.join(model_dir, \"model_scores.npy\"), allow_pickle=True).item()\n",
    "\n",
    "    # Load additional data if exists\n",
    "    additional_data_path = os.path.join(model_dir, \"additional_data.npy\")\n",
    "    if os.path.exists(additional_data_path):\n",
    "        model_data['additional_data'] = np.load(additional_data_path, allow_pickle=True).item()\n",
    "\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"- Components: {model_data['model_scores']['n_components']}\")\n",
    "    print(f\"- Classes: {len(model_data['classifier_params']['classes'])}\")\n",
    "    print(f\"- Accuracy: {model_data['aggregate_metrics']['accuracy']:.4f}\")\n",
    "\n",
    "    return model_data\n",
    "\n",
    "def plot_from_saved_model(model_data, plot_type='all'):\n",
    "    \"\"\"\n",
    "    Generate plots from saved model data\n",
    "\n",
    "    Parameters:\n",
    "    - model_data: loaded model data dictionary\n",
    "    - plot_type: type of plots to generate ('decision_boundary', 'contour', 'convergence', 'all')\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = model_data['X_train']\n",
    "    y_train = model_data['y_train']\n",
    "\n",
    "    # Reconstruct classifier for plotting (simplified version)\n",
    "    class SavedClassifierPlotter:\n",
    "        def __init__(self, model_data):\n",
    "            self.classes = model_data['classifier_params']['classes']\n",
    "            self.class_priors = model_data['classifier_params']['class_priors']\n",
    "            self.gmm_params = model_data['gmm_params']\n",
    "\n",
    "        def predict_proba(self, X):\n",
    "            n_samples = X.shape[0]\n",
    "            n_classes = len(self.classes)\n",
    "            probabilities = np.zeros((n_samples, n_classes))\n",
    "\n",
    "            for i, class_label in enumerate(self.classes):\n",
    "                class_params = self.gmm_params[class_label]\n",
    "                means = class_params['means']\n",
    "                covariances = class_params['covariances']\n",
    "                weights = class_params['weights']\n",
    "\n",
    "                # Compute class likelihood\n",
    "                likelihoods = np.zeros(n_samples)\n",
    "                for j in range(len(means)):\n",
    "                    component_likelihood = weights[j] * multivariate_gaussian_pdf(X, means[j], covariances[j])\n",
    "                    likelihoods += component_likelihood\n",
    "\n",
    "                probabilities[:, i] = likelihoods * self.class_priors[class_label]\n",
    "\n",
    "            # Normalize\n",
    "            total_prob = np.sum(probabilities, axis=1, keepdims=True)\n",
    "            total_prob[total_prob == 0] = 1e-10\n",
    "            probabilities = probabilities / total_prob\n",
    "\n",
    "            return probabilities\n",
    "\n",
    "        def predict(self, X):\n",
    "            probabilities = self.predict_proba(X)\n",
    "            predicted_indices = np.argmax(probabilities, axis=1)\n",
    "            return self.classes[predicted_indices]\n",
    "\n",
    "    classifier = SavedClassifierPlotter(model_data)\n",
    "    n_components = model_data['model_scores']['n_components']\n",
    "\n",
    "    if X_train.shape[1] == 2:  # Only for 2D data\n",
    "        if plot_type in ['decision_boundary', 'all']:\n",
    "            plot_decision_boundary(X_train, y_train, classifier, \n",
    "                                 title=f\"Decision Boundary ({n_components} components) - Loaded Model\")\n",
    "            plt.show()\n",
    "\n",
    "        if plot_type in ['contour', 'all']:\n",
    "            plot_contour_gmm(X_train, y_train, classifier,\n",
    "                           title=f\"GMM Contours ({n_components} components) - Loaded Model\")\n",
    "            plt.show()\n",
    "\n",
    "    if plot_type in ['convergence', 'all']:\n",
    "        # Plot convergence for each class\n",
    "        for class_label in model_data['classifier_params']['classes']:\n",
    "            log_likelihoods = model_data['gmm_params'][class_label]['log_likelihoods']\n",
    "            plot_log_likelihood_convergence(log_likelihoods,\n",
    "                                           title=f\"Convergence - Class {class_label} ({n_components} components)\")\n",
    "            plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENTAL PIPELINE WITH SAVING\n",
    "# ============================================================================\n",
    "\n",
    "def run_gmm_experiment_with_saving(X_train, y_train, X_test, y_test, \n",
    "                                  component_options=[1, 2, 4, 8, 16, 32, 64],\n",
    "                                  random_state=42, save_models=True, base_dir=\"saved_models\"):\n",
    "    \"\"\"\n",
    "    Run complete GMM experiment with model saving capability\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    saved_model_paths = {}\n",
    "\n",
    "    # Create base directory for saving models\n",
    "    if save_models:\n",
    "        base_path = create_model_save_directory(base_dir)\n",
    "        print(f\"Models will be saved to: {base_path}\")\n",
    "\n",
    "    print(\"Running GMM experiments with different component numbers...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for n_components in component_options:\n",
    "        print(f\"\\nTesting with {n_components} components...\")\n",
    "\n",
    "        # Train classifier\n",
    "        classifier = GMMBayesClassifier(\n",
    "            n_components_per_class=n_components,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = classifier.predict(X_test)\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = classification_metrics(y_test, y_pred)\n",
    "\n",
    "            # Compute model selection criteria\n",
    "            log_likelihood = classifier.get_log_likelihood(X_train, y_train)\n",
    "\n",
    "            # Count parameters\n",
    "            d = X_train.shape[1]\n",
    "            n_classes = len(np.unique(y_train))\n",
    "            params_per_component = d + (d * (d + 1)) // 2 + 1\n",
    "            total_params = n_classes * n_components * params_per_component\n",
    "\n",
    "\n",
    "            # Store results\n",
    "            results[n_components] = {\n",
    "                'classifier': classifier,\n",
    "                'metrics': metrics,\n",
    "                'log_likelihood': log_likelihood,\n",
    "                'n_params': total_params,\n",
    "                'y_pred': y_pred\n",
    "            }\n",
    "\n",
    "            # Save model if requested\n",
    "            if save_models:\n",
    "                # Prepare additional data for saving\n",
    "                additional_data = {\n",
    "                    'random_state': random_state,\n",
    "                    'component_options': component_options,\n",
    "                    'training_time': datetime.now().isoformat()\n",
    "                }\n",
    "\n",
    "                model_path = save_gmm_model(\n",
    "                    classifier=classifier,\n",
    "                    n_components=n_components,\n",
    "                    base_path=base_path,\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    X_test=X_test,\n",
    "                    y_test=y_test,\n",
    "                    y_pred=y_pred,\n",
    "                    metrics=metrics,\n",
    "                    log_likelihood=log_likelihood,\n",
    "                    additional_data=additional_data\n",
    "                )\n",
    "\n",
    "                saved_model_paths[n_components] = model_path\n",
    "\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Mean F1-Score: {metrics['mean_f1']:.4f}\")\n",
    "            print(f\"Log-Likelihood: {log_likelihood:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n_components} components: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if save_models:\n",
    "        # Save experiment summary\n",
    "        experiment_summary = {\n",
    "            'base_path': base_path,\n",
    "            'component_options': component_options,\n",
    "            'saved_model_paths': saved_model_paths,\n",
    "            'experiment_date': datetime.now().isoformat(),\n",
    "            'n_train_samples': len(X_train),\n",
    "            'n_test_samples': len(X_test),\n",
    "            'n_features': X_train.shape[1],\n",
    "            'n_classes': len(np.unique(y_train))\n",
    "        }\n",
    "\n",
    "        summary_path = os.path.join(base_path, \"experiment_summary.npy\")\n",
    "        np.save(summary_path, experiment_summary)\n",
    "\n",
    "        # Create experiment summary text file\n",
    "        summary_text_path = os.path.join(base_path, \"experiment_summary.txt\")\n",
    "        with open(summary_text_path, 'w') as f:\n",
    "            f.write(\"GMM Experiment Summary\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Experiment Date: {experiment_summary['experiment_date']}\\n\")\n",
    "            f.write(f\"Base Directory: {base_path}\\n\")\n",
    "            f.write(f\"Component Options: {component_options}\\n\\n\")\n",
    "\n",
    "            f.write(\"Dataset Information:\\n\")\n",
    "            f.write(f\"- Training Samples: {len(X_train)}\\n\")\n",
    "            f.write(f\"- Test Samples: {len(X_test)}\\n\")\n",
    "            f.write(f\"- Features: {X_train.shape[1]}\\n\")\n",
    "            f.write(f\"- Classes: {len(np.unique(y_train))}\\n\\n\")\n",
    "\n",
    "            f.write(\"Model Performance Summary:\\n\")\n",
    "            f.write(f\"{'Components':<12} {'Accuracy':<10} {'Mean F1':<10} \\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "            for n_comp in sorted(results.keys()):\n",
    "                result = results[n_comp]\n",
    "                metrics = result['metrics']\n",
    "                f.write(f\"{n_comp:<12} {metrics['accuracy']:<10.4f} \"\n",
    "                       f\"{metrics['mean_f1']:<10.4f}\\n\")\n",
    "\n",
    "        print(f\"\\nExperiment summary saved to: {summary_path}\")\n",
    "        print(f\"All models saved in: {base_path}\")\n",
    "\n",
    "        return results, saved_model_paths, base_path\n",
    "\n",
    "    return results\n",
    "\n",
    "def load_experiment_results(base_path):\n",
    "    \"\"\"Load complete experiment results from saved directory\"\"\"\n",
    "\n",
    "    print(f\"Loading experiment results from: {base_path}\")\n",
    "\n",
    "    # Load experiment summary\n",
    "    summary_path = os.path.join(base_path, \"experiment_summary.npy\")\n",
    "    if not os.path.exists(summary_path):\n",
    "        raise FileNotFoundError(f\"Experiment summary not found at {summary_path}\")\n",
    "\n",
    "    experiment_summary = np.load(summary_path, allow_pickle=True).item()\n",
    "\n",
    "    # Load all models\n",
    "    loaded_models = {}\n",
    "    for n_components, model_path in experiment_summary['saved_model_paths'].items():\n",
    "        loaded_models[n_components] = load_gmm_model(model_path)\n",
    "\n",
    "    print(f\"Loaded {len(loaded_models)} models from experiment\")\n",
    "\n",
    "    return loaded_models, experiment_summary\n",
    "\n",
    "def generate_plots_from_experiment(base_path, component_list=None, plot_types=['all']):\n",
    "    \"\"\"Generate plots for all or selected models from an experiment\"\"\"\n",
    "\n",
    "    loaded_models, experiment_summary = load_experiment_results(base_path)\n",
    "\n",
    "    if component_list is None:\n",
    "        component_list = list(loaded_models.keys())\n",
    "\n",
    "    print(f\"Generating plots for components: {component_list}\")\n",
    "\n",
    "    for n_components in component_list:\n",
    "        if n_components in loaded_models:\n",
    "            print(f\"\\nGenerating plots for {n_components} components...\")\n",
    "            model_data = loaded_models[n_components]\n",
    "\n",
    "            for plot_type in plot_types:\n",
    "                try:\n",
    "                    plot_from_saved_model(model_data, plot_type=plot_type)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating {plot_type} plot for {n_components} components: {e}\")\n",
    "        else:\n",
    "            print(f\"Model with {n_components} components not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# ORIGINAL EXPERIMENTAL PIPELINE (WITHOUT SAVING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_gmm_experiment(X_train, y_train, X_test, y_test, \n",
    "                      component_options=[1, 2, 4, 8, 16, 32, 64],\n",
    "                      random_state=42):\n",
    "    \"\"\"Run complete GMM experiment with different numbers of components\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    print(\"Running GMM experiments with different component numbers...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for n_components in component_options:\n",
    "        print(f\"\\nTesting with {n_components} components...\")\n",
    "\n",
    "        # Train classifier\n",
    "        classifier = GMMBayesClassifier(\n",
    "            n_components_per_class=n_components,\n",
    "            random_state=random_state\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            classifier.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = classifier.predict(X_test)\n",
    "\n",
    "            # Compute metrics\n",
    "            metrics = classification_metrics(y_test, y_pred)\n",
    "\n",
    "            # Compute model selection criteria\n",
    "            log_likelihood = classifier.get_log_likelihood(X_train, y_train)\n",
    "\n",
    "            # Count parameters\n",
    "            d = X_train.shape[1]\n",
    "            n_classes = len(np.unique(y_train))\n",
    "            params_per_component = d + (d * (d + 1)) // 2 + 1\n",
    "            total_params = n_classes * n_components * params_per_component\n",
    "\n",
    "\n",
    "            # Store results\n",
    "            results[n_components] = {\n",
    "                'classifier': classifier,\n",
    "                'metrics': metrics,\n",
    "                'log_likelihood': log_likelihood,\n",
    "                'n_params': total_params\n",
    "            }\n",
    "\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Mean F1-Score: {metrics['mean_f1']:.4f}\")\n",
    "            print(f\"Log-Likelihood: {log_likelihood:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {n_components} components: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n",
    "def create_results_table(results):\n",
    "    \"\"\"Create formatted results table\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"COMPREHENSIVE RESULTS TABLE\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    header = f\"{'Components':<12} {'Accuracy':<10} {'Mean Prec':<12} {'Mean Rec':<12} \" \\\n",
    "             f\"{'Mean F1':<10} {'Log-Lik':<12}\"\n",
    "    print(header)\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for n_components in sorted(results.keys()):\n",
    "        result = results[n_components]\n",
    "        metrics = result['metrics']\n",
    "\n",
    "        row = f\"{n_components:<12} {metrics['accuracy']:<10.4f} \" \\\n",
    "              f\"{metrics['mean_precision']:<12.4f} {metrics['mean_recall']:<12.4f} \" \\\n",
    "              f\"{metrics['mean_f1']:<10.4f} {result['log_likelihood']:<12.2f} \"\n",
    "        print(row)\n",
    "\n",
    "def find_best_model(results, criterion='f1'):\n",
    "    \"\"\"Find best model based on specified criterion\"\"\"\n",
    "    if not results:\n",
    "        return None\n",
    "\n",
    "    if criterion == 'accuracy':\n",
    "        best_components = max(results.keys(), key=lambda k: results[k]['metrics']['accuracy'])\n",
    "    elif criterion == 'f1':\n",
    "        best_components = max(results.keys(), key=lambda k: results[k]['metrics']['mean_f1'])\n",
    "    else:\n",
    "        raise ValueError(\"Criterion must be 'accuracy', or 'f1'\")\n",
    "\n",
    "    print(f\"\\nBest model ({criterion}): {best_components} components\")\n",
    "\n",
    "    return best_components, results[best_components]\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS FOR DATA HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "def train_test_split(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"Simple train-test split implementation\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_test = int(n_samples * test_size)\n",
    "\n",
    "    # Random permutation\n",
    "    indices = np.random.permutation(n_samples)\n",
    "\n",
    "    test_indices = indices[:n_test]\n",
    "    train_indices = indices[n_test:]\n",
    "\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def generate_sample_data():\n",
    "    \"\"\"Generate sample datasets for testing\"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create nonlinearly separable data\n",
    "    n_samples_per_class = 200\n",
    "\n",
    "    # Class 1: circular pattern\n",
    "    theta1 = np.linspace(0, 2*np.pi, n_samples_per_class)\n",
    "    r1 = 2 + 0.5 * np.random.randn(n_samples_per_class)\n",
    "    X1 = np.column_stack([r1 * np.cos(theta1), r1 * np.sin(theta1)])\n",
    "    y1 = np.zeros(n_samples_per_class)\n",
    "\n",
    "    # Class 2: inner circular pattern  \n",
    "    theta2 = np.linspace(0, 2*np.pi, n_samples_per_class)\n",
    "    r2 = 0.8 + 0.3 * np.random.randn(n_samples_per_class)\n",
    "    X2 = np.column_stack([r2 * np.cos(theta2), r2 * np.sin(theta2)])\n",
    "    y2 = np.ones(n_samples_per_class)\n",
    "\n",
    "    # Class 3: offset cluster\n",
    "    X3 = np.random.randn(n_samples_per_class, 2) * 0.5 + np.array([4, 4])\n",
    "    y3 = np.full(n_samples_per_class, 2)\n",
    "\n",
    "    # Combine data\n",
    "    X = np.vstack([X1, X2, X3])\n",
    "    y = np.concatenate([y1, y2, y3])\n",
    "\n",
    "    # Add some noise\n",
    "    X += np.random.randn(*X.shape) * 0.1\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "def main_example_with_saving():\n",
    "    \"\"\"Example usage of the complete implementation with model saving\"\"\"\n",
    "    print(\"GMM-Based Bayes Classifier - Complete Implementation with Model Saving\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Generate sample data\n",
    "#     X, y = generate_sample_data()\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "#     base_path = r'../../Dataset/Group04/NLS_Group04/'\n",
    "#     X_train, y_train, X_test, y_test, class_map = load_train_test_datasets(base_path)\n",
    "    X_train, y_train, class_labels = load_bovw_data(r\"../Dataset/Group04-SUN397/group04/train/\")\n",
    "    X_test, y_test, _ = load_bovw_data(r\"../Dataset/Group04-SUN397/group04/test/\")\n",
    "\n",
    "#     print(f\"Dataset: {len(X)} samples, {X.shape[1]} features, {len(np.unique(y))} classes\")\n",
    "    print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "    # Run experiments with model saving\n",
    "    results, saved_paths, base_path = run_gmm_experiment_with_saving(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        component_options=[ 2, 4, 8, 16,],\n",
    "        save_models=True\n",
    "    )\n",
    "\n",
    "    # Display results table\n",
    "    create_results_table(results)\n",
    "\n",
    "    # Find best model\n",
    "    best_components, best_result = find_best_model(results, criterion='f1')\n",
    "\n",
    "    # Print detailed report for best model\n",
    "    print_classification_report(best_result['metrics'])\n",
    "\n",
    "    print(f\"\\nModels saved to: {base_path}\")\n",
    "    print(\"You can later load models using:\")\n",
    "    print(f\"  loaded_models, summary = load_experiment_results('{base_path}')\")\n",
    "\n",
    "    return base_path\n",
    "\n",
    "def load_bovw_data(bovw_root):\n",
    "    X = []\n",
    "    y = []\n",
    "    class_names = sorted(os.listdir(bovw_root))\n",
    "\n",
    "    for class_label in class_names:\n",
    "        class_path = os.path.join(bovw_root, class_label,\"bovw\")\n",
    "\n",
    "        for file in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file)\n",
    "            vec = np.load(file_path)\n",
    "            X.append(vec)\n",
    "            y.append(class_label)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y, class_names\n",
    "\n",
    "def load_train_test_datasets(base_folder):\n",
    "    def load_folder(folder):\n",
    "        data, labels, class_map = [], [], {}\n",
    "        files = [f for f in os.listdir(folder) if f.endswith(\".txt\")]\n",
    "        files.sort()\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            class_name = file.split(\"_\")[0]\n",
    "            if class_name not in class_map:\n",
    "                class_map[class_name] = len(class_map)\n",
    "            label = class_map[class_name]\n",
    "            points = np.loadtxt(path, delimiter=' ')\n",
    "            data.append(points)\n",
    "            labels.append(np.full(len(points), label))\n",
    "        return np.vstack(data), np.hstack(labels), class_map\n",
    "\n",
    "    train_folder = os.path.join(base_folder, \"train\")\n",
    "    test_folder = os.path.join(base_folder, \"test\")\n",
    "    X_train, y_train, class_map = load_folder(train_folder)\n",
    "    X_test, y_test, _ = load_folder(test_folder)\n",
    "    return X_train, y_train, X_test, y_test, class_map\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_example_with_saving()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
