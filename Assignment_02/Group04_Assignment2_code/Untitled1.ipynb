{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f669ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc794b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "assignment2_groupXX.py\n",
    "GroupXX - Programming Assignment 2\n",
    "Implements:\n",
    " - K-Means (from scratch)\n",
    " - GMM (EM) with K-Means initialization (from scratch)\n",
    " - Evaluation metrics (accuracy, precision, recall, F1, confusion matrix)\n",
    " - Feature extraction for Dataset 2(b): 24-D patch histograms and BoVW (32 words)\n",
    " - Feature extraction for Dataset 2(c): 7x7 patch mean/std (2-D features)\n",
    " - Segmentation of cell images using K-Means and GMM\n",
    " - Plotting: contours, decision regions, iterations vs log-likelihood\n",
    "\n",
    "Dependencies: numpy, pandas, pillow (PIL), matplotlib\n",
    "Run: python assignment2_groupXX.py\n",
    "Adjust dataset paths below before running.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration - update paths\n",
    "# ----------------------------\n",
    "\n",
    "# Example folder structure expected:\n",
    "# datasets/\n",
    "#   dataset1/  <-- 2D data files per class as text or numpy arrays\n",
    "#       class1.npy\n",
    "#       class2.npy\n",
    "#   vowel/     <-- 2D speech dataset used in assignment1\n",
    "#       class1.npy ...\n",
    "#   scene/     <-- images per class in subfolders train/ test/\n",
    "#       train/\n",
    "#         class1/\n",
    "#           img1.jpg ...\n",
    "#       test/\n",
    "#         class1/\n",
    "#   cells/     <-- cell images for Dataset 2(c), train/ test folders\n",
    "#       train/\n",
    "#         img1.png ...\n",
    "#       test/\n",
    "DATA_ROOT = \"datasets\"   # change to actual path\n",
    "\n",
    "DATASET1_DIR = os.path.join(DATA_ROOT, \"dataset1\")\n",
    "VOWEL_DIR = os.path.join(DATA_ROOT, \"vowel\")\n",
    "SCENE_DIR = os.path.join(DATA_ROOT, \"scene\")\n",
    "CELLS_DIR = os.path.join(DATA_ROOT, \"cells\")\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = \"out_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Utility helpers\n",
    "# ----------------------------\n",
    "def train_test_split_per_class(X, labels, train_ratio=0.7):\n",
    "    \"\"\"\n",
    "    Splits X and labels ensuring each class has train_ratio train, rest test.\n",
    "    X: numpy array shape (N, D)\n",
    "    labels: array-like shape (N,)\n",
    "    returns: X_train, y_train, X_test, y_test\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "    classes = np.unique(labels)\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    for c in classes:\n",
    "        idx = np.where(labels == c)[0]\n",
    "        np.random.shuffle(idx)\n",
    "        n_train = int(len(idx) * train_ratio)\n",
    "        train_idx.extend(idx[:n_train])\n",
    "        test_idx.extend(idx[n_train:])\n",
    "    train_idx = np.array(train_idx)\n",
    "    test_idx = np.array(test_idx)\n",
    "    return X[train_idx], labels[train_idx], X[test_idx], labels[test_idx]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# K-Means (from scratch)\n",
    "# ----------------------------\n",
    "def kmeans(X, K, max_iters=100, tol=1e-5, verbose=False):\n",
    "    \"\"\"\n",
    "    X: (N, D)\n",
    "    returns: centroids (K, D), assignments (N,), inertia (sum squared distances)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    # initialize centroids: choose K random samples\n",
    "    idx = np.random.choice(N, K, replace=False)\n",
    "    centroids = X[idx].astype(float)\n",
    "    prev_inertia = None\n",
    "    for it in range(max_iters):\n",
    "        # assign\n",
    "        dists = np.sum((X[:, None, :] - centroids[None, :, :]) ** 2, axis=2)  # (N, K)\n",
    "        assignments = np.argmin(dists, axis=1)\n",
    "        inertia = np.sum(np.min(dists, axis=1))\n",
    "        if verbose:\n",
    "            print(f\"KMeans iter {it} inertia {inertia:.6f}\")\n",
    "        # update\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for k in range(K):\n",
    "            members = X[assignments == k]\n",
    "            if len(members) == 0:\n",
    "                # reinit empty centroid\n",
    "                new_centroids[k] = X[np.random.choice(N)]\n",
    "            else:\n",
    "                new_centroids[k] = members.mean(axis=0)\n",
    "        centroids = new_centroids\n",
    "        if prev_inertia is not None and abs(prev_inertia - inertia) < tol:\n",
    "            break\n",
    "        prev_inertia = inertia\n",
    "    return centroids, assignments, inertia\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Multivariate Gaussian helpers\n",
    "# ----------------------------\n",
    "def mvn_logpdf(X, mu, cov):\n",
    "    \"\"\"\n",
    "    Multivariate normal log pdf, robust to singular cov by adding small regularizer.\n",
    "    X: (N, D)\n",
    "    mu: (D,)\n",
    "    cov: (D, D)\n",
    "    returns: (N,) log pdf values\n",
    "    \"\"\"\n",
    "    D = X.shape[1]\n",
    "    eps = 1e-6\n",
    "    cov_reg = cov + eps * np.eye(D)\n",
    "    try:\n",
    "        chol = np.linalg.cholesky(cov_reg)\n",
    "        inv_cov = np.linalg.inv(cov_reg)\n",
    "        det_cov = np.prod(np.diag(chol)) ** 2\n",
    "    except np.linalg.LinAlgError:\n",
    "        # fallback to pseudo-inverse\n",
    "        inv_cov = np.linalg.pinv(cov_reg)\n",
    "        det_cov = np.linalg.det(cov_reg) if np.linalg.det(cov_reg) > 0 else eps\n",
    "    xm = X - mu\n",
    "    # Mahalanobis\n",
    "    m = np.sum(xm @ inv_cov * xm, axis=1)\n",
    "    log_norm = -0.5 * (D * np.log(2 * np.pi) + np.log(det_cov) + m)\n",
    "    return log_norm\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# GMM (EM) from scratch\n",
    "# ----------------------------\n",
    "class GMM:\n",
    "    def __init__(self, n_components=1, max_iters=100, tol=1e-4, verbose=False):\n",
    "        self.K = n_components\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "        # parameters\n",
    "        self.weights_ = None  # (K,)\n",
    "        self.means_ = None    # (K, D)\n",
    "        self.covs_ = None     # (K, D, D)\n",
    "        self.log_likelihoods_ = []\n",
    "\n",
    "    def init_params_kmeans(self, X):\n",
    "        # initialize using KMeans\n",
    "        centroids, assignments, _ = kmeans(X, self.K, max_iters=50)\n",
    "        N, D = X.shape\n",
    "        weights = np.zeros(self.K)\n",
    "        covs = np.zeros((self.K, D, D))\n",
    "        means = centroids.copy()\n",
    "        for k in range(self.K):\n",
    "            members = X[assignments == k]\n",
    "            weights[k] = max(1, len(members))\n",
    "            if len(members) <= 1:\n",
    "                covs[k] = np.cov(X.T) if N > 1 else np.eye(D)\n",
    "            else:\n",
    "                covs[k] = np.cov(members.T)\n",
    "        weights = weights / np.sum(weights)\n",
    "        self.weights_ = weights\n",
    "        self.means_ = means\n",
    "        # make covs positive definite\n",
    "        for k in range(self.K):\n",
    "            if covs[k].shape == ():\n",
    "                covs[k] = np.eye(D)\n",
    "            else:\n",
    "                # regularize\n",
    "                covs[k] = covs[k] + 1e-6 * np.eye(D)\n",
    "        self.covs_ = covs\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = np.asarray(X)\n",
    "        N, D = X.shape\n",
    "        if self.K == 1:\n",
    "            # special case: single gaussian\n",
    "            self.means_ = np.mean(X, axis=0, keepdims=True)\n",
    "            cov = np.cov(X.T) + 1e-6 * np.eye(D)\n",
    "            self.covs_ = cov.reshape(1, D, D)\n",
    "            self.weights_ = np.array([1.0])\n",
    "            # compute one log-likelihood\n",
    "            lp = mvn_logpdf(X, self.means_[0], self.covs_[0])\n",
    "            self.log_likelihoods_ = [np.sum(lp)]\n",
    "            return self\n",
    "        # init\n",
    "        self.init_params_kmeans(X)\n",
    "        prev_ll = None\n",
    "        for it in range(self.max_iters):\n",
    "            # E-step: compute responsibilities\n",
    "            log_prob = np.zeros((N, self.K))\n",
    "            for k in range(self.K):\n",
    "                log_prob[:, k] = np.log(self.weights_[k] + 1e-12) + mvn_logpdf(X, self.means_[k], self.covs_[k])\n",
    "            # log-sum-exp for numeric stability\n",
    "            max_log = np.max(log_prob, axis=1, keepdims=True)\n",
    "            log_resp = log_prob - max_log - np.log(np.sum(np.exp(log_prob - max_log), axis=1, keepdims=True) + 1e-12)\n",
    "            resp = np.exp(log_resp)\n",
    "            Nk = resp.sum(axis=0) + 1e-12  # (K,)\n",
    "            # M-step\n",
    "            weights = Nk / N\n",
    "            means = (resp.T @ X) / Nk[:, None]\n",
    "            covs = np.zeros((self.K, D, D))\n",
    "            for k in range(self.K):\n",
    "                xm = X - means[k]\n",
    "                # weighted covariance\n",
    "                covs[k] = (resp[:, k][:, None] * xm).T @ xm / Nk[k]\n",
    "                covs[k] += 1e-6 * np.eye(D)  # regularize\n",
    "            self.weights_ = weights\n",
    "            self.means_ = means\n",
    "            self.covs_ = covs\n",
    "            # LL\n",
    "            ll = np.sum(np.log(np.sum(np.exp(log_prob), axis=1) + 1e-12))\n",
    "            self.log_likelihoods_.append(ll)\n",
    "            if self.verbose:\n",
    "                print(f\"GMM iter {it} ll={ll:.6f}\")\n",
    "            if prev_ll is not None and abs(ll - prev_ll) < self.tol:\n",
    "                break\n",
    "            prev_ll = ll\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = np.asarray(X)\n",
    "        N = X.shape[0]\n",
    "        log_prob = np.zeros((N, self.K))\n",
    "        for k in range(self.K):\n",
    "            log_prob[:, k] = np.log(self.weights_[k] + 1e-12) + mvn_logpdf(X, self.means_[k], self.covs_[k])\n",
    "        # normalize\n",
    "        max_log = np.max(log_prob, axis=1, keepdims=True)\n",
    "        log_prob_norm = log_prob - max_log - np.log(np.sum(np.exp(log_prob - max_log), axis=1, keepdims=True) + 1e-12)\n",
    "        resp = np.exp(log_prob_norm)\n",
    "        return resp  # responsibilities (N, K)\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        # return log p(x)\n",
    "        X = np.asarray(X)\n",
    "        N = X.shape[0]\n",
    "        log_prob = np.zeros((N, self.K))\n",
    "        for k in range(self.K):\n",
    "            log_prob[:, k] = np.log(self.weights_[k] + 1e-12) + mvn_logpdf(X, self.means_[k], self.covs_[k])\n",
    "        # log-sum-exp\n",
    "        max_log = np.max(log_prob, axis=1, keepdims=True)\n",
    "        log_px = max_log.flatten() + np.log(np.sum(np.exp(log_prob - max_log), axis=1) + 1e-12)\n",
    "        return log_px\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Multiclass classification with class-conditional GMMs (Bayes classifier)\n",
    "# ----------------------------\n",
    "class BayesGMMClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_gmms = {}  # label -> GMM\n",
    "        self.class_priors = {}  # label -> prior\n",
    "\n",
    "    def fit(self, X, y, n_components_per_class):\n",
    "        \"\"\"\n",
    "        X: (N, D), y: (N,)\n",
    "        n_components_per_class: dict or int; if int same for all classes\n",
    "        \"\"\"\n",
    "        labels = np.unique(y)\n",
    "        N = X.shape[0]\n",
    "        for label in labels:\n",
    "            Xc = X[y == label]\n",
    "            ncomp = n_components_per_class[label] if isinstance(n_components_per_class, dict) else n_components_per_class\n",
    "            gmm = GMM(n_components=ncomp, max_iters=200, tol=1e-4, verbose=False)\n",
    "            gmm.fit(Xc)\n",
    "            self.class_gmms[label] = gmm\n",
    "            self.class_priors[label] = Xc.shape[0] / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        labels = sorted(list(self.class_gmms.keys()))\n",
    "        N = X.shape[0]\n",
    "        log_post = np.zeros((N, len(labels)))\n",
    "        for i, label in enumerate(labels):\n",
    "            gmm = self.class_gmms[label]\n",
    "            log_px = gmm.score_samples(X)  # log p(x | class)\n",
    "            log_prior = math.log(self.class_priors[label] + 1e-12)\n",
    "            log_post[:, i] = log_px + log_prior\n",
    "        idx = np.argmax(log_post, axis=1)\n",
    "        return np.array([labels[i] for i in idx])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        labels = sorted(list(self.class_gmms.keys()))\n",
    "        N = X.shape[0]\n",
    "        log_post = np.zeros((N, len(labels)))\n",
    "        for i, label in enumerate(labels):\n",
    "            gmm = self.class_gmms[label]\n",
    "            log_px = gmm.score_samples(X)\n",
    "            log_prior = math.log(self.class_priors[label] + 1e-12)\n",
    "            log_post[:, i] = log_px + log_prior\n",
    "        # normalize to get probabilities\n",
    "        max_log = np.max(log_post, axis=1, keepdims=True)\n",
    "        probs = np.exp(log_post - max_log) / np.sum(np.exp(log_post - max_log), axis=1, keepdims=True)\n",
    "        return probs, sorted(list(self.class_gmms.keys()))\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation metrics\n",
    "# ----------------------------\n",
    "def confusion_matrix(y_true, y_pred, labels=None):\n",
    "    if labels is None:\n",
    "        labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    label_to_idx = {l: i for i, l in enumerate(labels)}\n",
    "    cm = np.zeros((len(labels), len(labels)), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[label_to_idx[t], label_to_idx[p]] += 1\n",
    "    return cm, labels\n",
    "\n",
    "def precision_recall_f1(cm):\n",
    "    # cm: rows true, cols pred\n",
    "    TP = np.diag(cm).astype(float)\n",
    "    precision = TP / (cm.sum(axis=0) + 1e-12)\n",
    "    recall = TP / (cm.sum(axis=1) + 1e-12)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-12)\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_f1 = np.mean(f1)\n",
    "    return precision, recall, f1, mean_precision, mean_recall, mean_f1\n",
    "\n",
    "def accuracy_from_cm(cm):\n",
    "    return np.sum(np.diag(cm)) / np.sum(cm)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Plotting utilities (2D only)\n",
    "# ----------------------------\n",
    "def plot_gmm_contours_2d(gmm_dict, X_train, y_train, out_path):\n",
    "    \"\"\"\n",
    "    Plot constant density contours for each class GMM on same axes, superpose training data.\n",
    "    gmm_dict: {label: GMM}\n",
    "    \"\"\"\n",
    "    labels = sorted(list(gmm_dict.keys()))\n",
    "    # create grid covering data\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "    grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in labels:\n",
    "        gmm = gmm_dict[label]\n",
    "        logp = gmm.score_samples(grid)\n",
    "        p = np.exp(logp - np.max(logp))\n",
    "        zz = p.reshape(xx.shape)\n",
    "        plt.contour(xx, yy, zz, levels=6, alpha=0.7, linewidths=1.0, label=str(label))\n",
    "    # plot training points\n",
    "    for label in labels:\n",
    "        pts = X_train[y_train == label]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=f\"class {label}\", s=10)\n",
    "    plt.legend()\n",
    "    plt.title(\"Constant density contours (per-class GMM) + training data\")\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_decision_regions(classifier, X_train, y_train, out_path, resolution=200):\n",
    "    labels = np.unique(y_train)\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution))\n",
    "    grid = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    preds = classifier.predict(grid)\n",
    "    label_to_int = {lab: i for i, lab in enumerate(np.unique(y_train))}\n",
    "    Z = np.array([label_to_int[p] for p in preds]).reshape(xx.shape)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    for lab in np.unique(y_train):\n",
    "        pts = X_train[y_train == lab]\n",
    "        plt.scatter(pts[:, 0], pts[:, 1], label=str(lab), s=10)\n",
    "    plt.legend()\n",
    "    plt.title(\"Decision regions (training data superposed)\")\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Feature extraction for Dataset 2(b) - color histogram & BoVW\n",
    "# ----------------------------\n",
    "def extract_patch_color_histograms(image_path, patch_size=32):\n",
    "    \"\"\"\n",
    "    For an image, split into non-overlapping patch_size x patch_size patches.\n",
    "    For each patch compute 8-bin hist per channel (R,G,B), normalized -> 24D vector.\n",
    "    Returns array of shape (num_patches, 24)\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    arr = np.array(img)  # H x W x 3\n",
    "    H, W, _ = arr.shape\n",
    "    ph = patch_size\n",
    "    vectors = []\n",
    "    for y in range(0, H, ph):\n",
    "        for x in range(0, W, ph):\n",
    "            if y + ph <= H and x + ph <= W:\n",
    "                patch = arr[y:y+ph, x:x+ph]  # ph x ph x 3\n",
    "                # per channel hist (0-255 into 8 bins)\n",
    "                patch_vec = []\n",
    "                for ch in range(3):\n",
    "                    channel = patch[:, :, ch].flatten()\n",
    "                    hist, _ = np.histogram(channel, bins=8, range=(0, 256))\n",
    "                    hist = hist.astype(float) / (ph * ph)\n",
    "                    patch_vec.extend(hist.tolist())\n",
    "                vectors.append(patch_vec)\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros((0, 24))\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "\n",
    "def build_bovw_codebook(all_patch_vectors, n_words=32):\n",
    "    \"\"\"\n",
    "    all_patch_vectors: (M, 24) np array from all training images across classes\n",
    "    returns: codebook centers (n_words, 24)\n",
    "    \"\"\"\n",
    "    centers, _, _ = kmeans(all_patch_vectors, n_words, max_iters=100)\n",
    "    return centers\n",
    "\n",
    "\n",
    "def image_to_bovw(image_patch_vectors, codebook):\n",
    "    \"\"\"\n",
    "    Assign each patch vector to nearest codebook center and compute normalized histogram (n_words)\n",
    "    \"\"\"\n",
    "    if image_patch_vectors.shape[0] == 0:\n",
    "        return np.zeros(codebook.shape[0])\n",
    "    dists = np.sum((image_patch_vectors[:, None, :] - codebook[None, :, :])**2, axis=2)\n",
    "    assigns = np.argmin(dists, axis=1)\n",
    "    counts = np.bincount(assigns, minlength=codebook.shape[0]).astype(float)\n",
    "    counts /= counts.sum()\n",
    "    return counts\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Feature extraction for Dataset 2(c) - cell images (7x7 overlapping patches)\n",
    "# ----------------------------\n",
    "def extract_7x7_mean_std(image_path):\n",
    "    \"\"\"\n",
    "    For grayscale cell image, take overlapping 7x7 patches with stride 1,\n",
    "    compute mean and std of intensity -> 2D vector per patch.\n",
    "    Return stacked 2D features for the image (num_patches, 2)\n",
    "    \"\"\"\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    arr = np.array(img).astype(float)\n",
    "    H, W = arr.shape\n",
    "    ph = 7\n",
    "    vectors = []\n",
    "    for y in range(0, H - ph + 1):\n",
    "        for x in range(0, W - ph + 1):\n",
    "            patch = arr[y:y+ph, x:x+ph]\n",
    "            mn = np.mean(patch)\n",
    "            sd = np.std(patch)\n",
    "            vectors.append([mn, sd])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros((0, 2))\n",
    "    return np.vstack(vectors)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset loaders (lightweight)\n",
    "# ----------------------------\n",
    "def load_2d_dataset(dataset1_dir):\n",
    "    \"\"\"\n",
    "    Expects class files as numpy .npy arrays with shape (N_i, 2) in dataset1_dir.\n",
    "    Returns X (N,2), y (N,)\n",
    "    \"\"\"\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for fname in os.listdir(dataset1_dir):\n",
    "        if fname.endswith(\".npy\") or fname.endswith(\".npz\") or fname.endswith(\".csv\") or fname.endswith(\".txt\"):\n",
    "            label = os.path.splitext(fname)[0]\n",
    "            path = os.path.join(dataset1_dir, fname)\n",
    "            if fname.endswith(\".npy\"):\n",
    "                data = np.load(path)\n",
    "            else:\n",
    "                data = np.loadtxt(path, delimiter=',')\n",
    "            if data.ndim == 1:\n",
    "                data = data.reshape(-1, 2)\n",
    "            Xs.append(data)\n",
    "            ys.extend([label]*data.shape[0])\n",
    "    if len(Xs) == 0:\n",
    "        raise RuntimeError(\"No files found in dataset1_dir\")\n",
    "    X = np.vstack(Xs)\n",
    "    y = np.array(ys)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def load_vowel_2d(vowel_dir):\n",
    "    \"\"\"\n",
    "    Assume vowel dataset stored similarly as per-class .npy files (2D features).\n",
    "    \"\"\"\n",
    "    return load_2d_dataset(vowel_dir)\n",
    "\n",
    "\n",
    "def load_scene_images(scene_dir, split='train'):\n",
    "    \"\"\"\n",
    "    Expects scene_dir/train/<class>/*.jpg  and scene_dir/test/<class>/*.jpg\n",
    "    Returns a list of (image_path, class_label)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    folder = os.path.join(scene_dir, split)\n",
    "    for cls in os.listdir(folder):\n",
    "        cls_folder = os.path.join(folder, cls)\n",
    "        if not os.path.isdir(cls_folder): continue\n",
    "        for fname in os.listdir(cls_folder):\n",
    "            if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "                out.append((os.path.join(cls_folder, fname), cls))\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_cells_images(cells_dir, split='train'):\n",
    "    folder = os.path.join(cells_dir, split)\n",
    "    images = []\n",
    "    for fname in os.listdir(folder):\n",
    "        if fname.lower().endswith(('.jpg', '.png', '.jpeg', '.bmp')):\n",
    "            images.append((os.path.join(folder, fname)))\n",
    "    return images\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# High-level pipeline functions\n",
    "# ----------------------------\n",
    "def run_classification_dataset1_and_vowel(X, y, dataset_name=\"dataset1\", mixtures=[1,2,4,8,16,32,64]):\n",
    "    \"\"\"\n",
    "    For Dataset1 (2D) and Dataset2(a) vowel data: build Bayes classifier with GMM per class.\n",
    "    Try different number of mixtures and report metrics.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # train/test split per class 70/30\n",
    "    X_train, y_train, X_test, y_test = train_test_split_per_class(X, y, train_ratio=0.7)\n",
    "    for ncomp in mixtures:\n",
    "        print(f\"Training Bayes GMM with {ncomp} components per class...\")\n",
    "        clf = BayesGMMClassifier()\n",
    "        # pass an int -> same for all classes\n",
    "        clf.fit(X_train, y_train, ncomp)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cm, labels = confusion_matrix(y_test, y_pred)\n",
    "        acc = accuracy_from_cm(cm)\n",
    "        prec, rec, f1, mprec, mrec, mf1 = precision_recall_f1(cm)\n",
    "        results[ncomp] = {\n",
    "            \"classifier\": clf,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"labels\": labels,\n",
    "            \"accuracy\": acc,\n",
    "            \"precision_per_class\": prec,\n",
    "            \"recall_per_class\": rec,\n",
    "            \"f1_per_class\": f1,\n",
    "            \"mean_precision\": mprec,\n",
    "            \"mean_recall\": mrec,\n",
    "            \"mean_f1\": mf1,\n",
    "            \"log_likelihoods\": {lab: clf.class_gmms[lab].log_likelihoods_ for lab in clf.class_gmms}\n",
    "        }\n",
    "        print(f\"Mixtures {ncomp}: acc={acc:.4f}, mean_f1={mf1:.4f}\")\n",
    "        # Save plots for best later; we'll choose best model by accuracy or mean_f1\n",
    "    # pick best model by mean_f1\n",
    "    best_n = max(results.keys(), key=lambda k: results[k]['mean_f1'])\n",
    "    best = results[best_n]\n",
    "    # contours & decision regions only for 2D data\n",
    "    try:\n",
    "        out_contour = os.path.join(OUT_DIR, f\"{dataset_name}_gmm_contours_best_{best_n}.png\")\n",
    "        # prepare dict label->GMM\n",
    "        gmm_dict = {lab: best['classifier'].class_gmms[lab] for lab in best['classifier'].class_gmms}\n",
    "        plot_gmm_contours_2d(gmm_dict, X_train, y_train, out_contour)\n",
    "        out_dec = os.path.join(OUT_DIR, f\"{dataset_name}_decision_regions_best_{best_n}.png\")\n",
    "        plot_decision_regions(best['classifier'], X_train, y_train, out_dec)\n",
    "    except Exception as e:\n",
    "        print(\"Plotting failed (likely not 2D):\", e)\n",
    "    # save results\n",
    "    with open(os.path.join(OUT_DIR, f\"{dataset_name}_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(results, f)\n",
    "    return results, best_n\n",
    "\n",
    "\n",
    "def run_scene_classification(scene_dir, mixtures=[1,2,4,8,16]):\n",
    "    \"\"\"\n",
    "    For Dataset 2(b) scene images:\n",
    "    - Extract 24D patch histograms for every image patch\n",
    "    - Build BoVW codebook (32 words) using KMeans on all training patch vectors\n",
    "    - For each image compute 24D-per-patch representation saved and BoVW 32D vector\n",
    "    - Train Bayes classifier using GMM on (i) color-hist patch-sets? (But classifier expects vector per image)\n",
    "      Assignment asks to build Bayes classifier using GMM on 24-D histogram feature vectors and on 32-D BoVW separately.\n",
    "      The 24-D representation: The assignment says \"For Dataset 2(b) using set of 24-dim colour histogram feature vectors and 32-dim BoVW feature vector separately.\"\n",
    "      Interpretation: For 24-D, represent each image as collection of patch vectors; but classifier needs fixed-size input per sample.\n",
    "      We'll interpret 24-D experiment as training GMM on patch-level data per class and classify patches then majority-vote per image.\n",
    "      Simpler & consistent approach: -- For 24-D: average the patch histograms per image -> 24D image descriptor.\n",
    "    \"\"\"\n",
    "    # load train/test lists\n",
    "    train_list = load_scene_images(scene_dir, split='train')\n",
    "    test_list = load_scene_images(scene_dir, split='test')\n",
    "    # build patch vectors for all training images\n",
    "    print(\"Extracting patch histograms for train images...\")\n",
    "    train_patch_vectors_per_image = []\n",
    "    train_labels = []\n",
    "    all_train_patches = []\n",
    "    for img_path, cls in train_list:\n",
    "        patches = extract_patch_color_histograms(img_path, patch_size=32)  # (num_patches,24)\n",
    "        train_patch_vectors_per_image.append(patches)\n",
    "        train_labels.append(cls)\n",
    "        if patches.shape[0] > 0:\n",
    "            all_train_patches.append(patches)\n",
    "    if len(all_train_patches) == 0:\n",
    "        raise RuntimeError(\"No training patches found.\")\n",
    "    all_train_patches = np.vstack(all_train_patches)\n",
    "    print(f\"Total training patches: {all_train_patches.shape[0]}\")\n",
    "    # build BoVW codebook\n",
    "    codebook = build_bovw_codebook(all_train_patches, n_words=32)\n",
    "    np.save(os.path.join(OUT_DIR, \"scene_codebook32.npy\"), codebook)\n",
    "    # compute image-level descriptors:\n",
    "    def image_24d_descriptor(patches):\n",
    "        if patches.shape[0] == 0:\n",
    "            return np.zeros(24)\n",
    "        return patches.mean(axis=0)\n",
    "    X_train_24 = np.vstack([image_24d_descriptor(p) for p in train_patch_vectors_per_image])\n",
    "    y_train = np.array(train_labels)\n",
    "    # build BoVW descriptors\n",
    "    X_train_bovw = np.vstack([image_to_bovw(p, codebook) for p in train_patch_vectors_per_image])\n",
    "    # test descriptors\n",
    "    test_patch_vectors_per_image = []\n",
    "    test_labels = []\n",
    "    for img_path, cls in test_list:\n",
    "        patches = extract_patch_color_histograms(img_path, patch_size=32)\n",
    "        test_patch_vectors_per_image.append(patches)\n",
    "        test_labels.append(cls)\n",
    "    X_test_24 = np.vstack([image_24d_descriptor(p) for p in test_patch_vectors_per_image])\n",
    "    X_test_bovw = np.vstack([image_to_bovw(p, codebook) for p in test_patch_vectors_per_image])\n",
    "    y_test = np.array(test_labels)\n",
    "    # Now run classification experiments separately for 24D and 32D\n",
    "    results_24 = {}\n",
    "    results_bovw = {}\n",
    "    for ncomp in mixtures:\n",
    "        print(f\"Training on 24D descriptors with {ncomp} mixtures...\")\n",
    "        clf24 = BayesGMMClassifier()\n",
    "        clf24.fit(X_train_24, y_train, ncomp)\n",
    "        y_pred24 = clf24.predict(X_test_24)\n",
    "        cm24, labels24 = confusion_matrix(y_test, y_pred24)\n",
    "        acc24 = accuracy_from_cm(cm24)\n",
    "        prec24, rec24, f124, mprec24, mrec24, mf124 = precision_recall_f1(cm24)\n",
    "        results_24[ncomp] = {\"clf\": clf24, \"cm\": cm24, \"labels\": labels24, \"acc\": acc24, \"mean_f1\": mf124}\n",
    "\n",
    "        print(f\"Training on BoVW(32D) with {ncomp} mixtures...\")\n",
    "        clfb = BayesGMMClassifier()\n",
    "        clfb.fit(X_train_bovw, y_train, ncomp)\n",
    "        y_predb = clfb.predict(X_test_bovw)\n",
    "        cmb, labelsb = confusion_matrix(y_test, y_predb)\n",
    "        accb = accuracy_from_cm(cmb)\n",
    "        precb, recb, f1b, mprecb, mrecb, mf1b = precision_recall_f1(cmb)\n",
    "        results_bovw[ncomp] = {\"clf\": clfb, \"cm\": cmb, \"labels\": labelsb, \"acc\": accb, \"mean_f1\": mf1b}\n",
    "\n",
    "        print(f\"24D: acc={acc24:.4f} mean_f1={mf124:.4f} | BoVW: acc={accb:.4f} mean_f1={mf1b:.4f}\")\n",
    "    # save results\n",
    "    with open(os.path.join(OUT_DIR, \"scene_results.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({\"24D\": results_24, \"BoVW\": results_bovw, \"codebook\": codebook}, f)\n",
    "    return {\"24D\": results_24, \"BoVW\": results_bovw, \"codebook\": codebook}\n",
    "\n",
    "\n",
    "def run_cell_segmentation(cells_dir, out_segment_dir, k_clust=3):\n",
    "    \"\"\"\n",
    "    For Dataset 2(c):\n",
    "    - For each training cell image extract 7x7 patch mean/std (2D) stacked file\n",
    "    - Stack all training patch vectors per class? Assignment asks to cluster local feature vectors into 3 groups.\n",
    "      We'll cluster the combined patches for each image (segmentation) and also run KMeans/GMM for the dataset overall.\n",
    "    - Do segmentation for test images by assigning each patch to cluster and create segmentation map.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_segment_dir, exist_ok=True)\n",
    "    train_folder = os.path.join(cells_dir, \"train\")\n",
    "    test_folder = os.path.join(cells_dir, \"test\")\n",
    "    # gather all training patch vectors across all images (stacked)\n",
    "    all_train_patches = []\n",
    "    train_image_files = []\n",
    "    for fname in os.listdir(train_folder):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            path = os.path.join(train_folder, fname)\n",
    "            v = extract_7x7_mean_std(path)\n",
    "            if v.shape[0] > 0:\n",
    "                all_train_patches.append(v)\n",
    "                train_image_files.append(path)\n",
    "    if len(all_train_patches) == 0:\n",
    "        raise RuntimeError(\"No training cell patches found.\")\n",
    "    stacked = np.vstack(all_train_patches)\n",
    "    print(f\"Total cell patches stacked: {stacked.shape}\")\n",
    "    # KMeans clustering into 3 groups\n",
    "    centers_km, assigns, _ = kmeans(stacked, k_clust, max_iters=200)\n",
    "    # GMM clustering (initialize using KMeans centers - we will subset patches per cluster for init)\n",
    "    gmm = GMM(n_components=k_clust, max_iters=200, tol=1e-4, verbose=True)\n",
    "    # prepare initial params using kmeans assignments\n",
    "    # For simplicity, compute cluster-wise cov/means\n",
    "    # We'll directly set parameters:\n",
    "    D = stacked.shape[1]\n",
    "    means = centers_km\n",
    "    covs = np.zeros((k_clust, D, D))\n",
    "    weights = np.zeros(k_clust)\n",
    "    for k in range(k_clust):\n",
    "        mem = stacked[assigns == k]\n",
    "        weights[k] = max(1, mem.shape[0])\n",
    "        if mem.shape[0] <= 1:\n",
    "            covs[k] = np.cov(stacked.T) + 1e-6 * np.eye(D)\n",
    "        else:\n",
    "            covs[k] = np.cov(mem.T) + 1e-6 * np.eye(D)\n",
    "    weights = weights / np.sum(weights)\n",
    "    gmm.weights_ = weights\n",
    "    gmm.means_ = means\n",
    "    gmm.covs_ = covs\n",
    "    gmm.fit(stacked)\n",
    "    # For every train image, produce segmentation maps by assigning each patch to KMeans and GMM\n",
    "    # Here we will reconstruct segmentation back to image grid shape. For 7x7 overlapping patches,\n",
    "    # we can place cluster label at patch top-left location to produce a map slightly smaller than original.\n",
    "    def segment_image_with_assignments(image_path, model_assigner='kmeans'):\n",
    "        img = Image.open(image_path).convert('L')\n",
    "        arr = np.array(img).astype(float)\n",
    "        H, W = arr.shape\n",
    "        ph = 7\n",
    "        seg_map = np.zeros((H - ph + 1, W - ph + 1), dtype=int)\n",
    "        idx = 0\n",
    "        for y in range(0, H - ph + 1):\n",
    "            for x in range(0, W - ph + 1):\n",
    "                patch = arr[y:y+ph, x:x+ph]\n",
    "                vec = np.array([np.mean(patch), np.std(patch)])[None, :]\n",
    "                if model_assigner == 'kmeans':\n",
    "                    d = np.sum((vec - centers_km)**2, axis=1)\n",
    "                    seg_map[y, x] = np.argmin(d)\n",
    "                else:\n",
    "                    # gmm: choose cluster with highest responsibility\n",
    "                    resp = gmm.predict_proba(vec)[0]\n",
    "                    seg_map[y, x] = np.argmax(resp)\n",
    "                idx += 1\n",
    "        return seg_map\n",
    "    # save segmentation for test images\n",
    "    for fname in os.listdir(test_folder):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "            path = os.path.join(test_folder, fname)\n",
    "            seg_km = segment_image_with_assignments(path, 'kmeans')\n",
    "            seg_gmm = segment_image_with_assignments(path, 'gmm')\n",
    "            # Save color-coded segmentation maps - simple: multiply labels by 80 to create grayscale\n",
    "            seg_km_img = (seg_km.astype(np.uint8) * (255 // max(1, k_clust-1)))\n",
    "            seg_gmm_img = (seg_gmm.astype(np.uint8) * (255 // max(1, k_clust-1)))\n",
    "            im_km = Image.fromarray(seg_km_img)\n",
    "            im_gmm = Image.fromarray(seg_gmm_img)\n",
    "            base = os.path.splitext(os.path.basename(fname))[0]\n",
    "            im_km.save(os.path.join(out_segment_dir, f\"{base}_km_seg.png\"))\n",
    "            im_gmm.save(os.path.join(out_segment_dir, f\"{base}_gmm_seg.png\"))\n",
    "    # also save a sample plot of clusters on training patch set (2D scatter) - because features are 2D\n",
    "    plt.figure(figsize=(6,6))\n",
    "    assigns_km = assigns\n",
    "    plt.scatter(stacked[:,0], stacked[:,1], c=assigns_km, s=2)\n",
    "    plt.title(\"KMeans clusters on cell patches (mean vs std)\")\n",
    "    plt.xlabel(\"mean\")\n",
    "    plt.ylabel(\"std\")\n",
    "    plt.savefig(os.path.join(out_segment_dir, \"cell_patches_kmeans_clusters.png\"))\n",
    "    plt.close()\n",
    "    # Save GMM predicted cluster assignment (hard)\n",
    "    resp = gmm.predict_proba(stacked)\n",
    "    hard = np.argmax(resp, axis=1)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(stacked[:,0], stacked[:,1], c=hard, s=2)\n",
    "    plt.title(\"GMM clusters on cell patches (mean vs std)\")\n",
    "    plt.savefig(os.path.join(out_segment_dir, \"cell_patches_gmm_clusters.png\"))\n",
    "    plt.close()\n",
    "    # Save models\n",
    "    with open(os.path.join(out_segment_dir, \"cell_kmeans_centers.npy\"), \"wb\") as f:\n",
    "        np.save(f, centers_km)\n",
    "    with open(os.path.join(out_segment_dir, \"cell_gmm.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(gmm, f)\n",
    "    print(\"Segmentation outputs saved to\", out_segment_dir)\n",
    "    return {\"kmeans_centers\": centers_km, \"gmm\": gmm}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example entrypoint to run all required experiments\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # 1) Dataset 1 (2D nonlinearly separable) - load and run\n",
    "    try:\n",
    "        print(\"Loading Dataset1...\")\n",
    "        X1, y1 = load_2d_dataset(DATASET1_DIR)\n",
    "        print(\"Running GMM experiments for Dataset1...\")\n",
    "        results1, best_n1 = run_classification_dataset1_and_vowel(X1, y1, dataset_name=\"dataset1\",\n",
    "                                                                  mixtures=[1,2,4,8,16,32])\n",
    "        print(f\"Dataset1 done. Best mixtures: {best_n1}\")\n",
    "    except Exception as e:\n",
    "        print(\"Dataset1 step skipped due to error:\", e)\n",
    "\n",
    "    # 2) Dataset 2(a) vowel 2D dataset\n",
    "    try:\n",
    "        print(\"Loading vowel dataset...\")\n",
    "        Xv, yv = load_vowel_2d(VOWEL_DIR)\n",
    "        print(\"Running GMM experiments for Vowel dataset...\")\n",
    "        results_v, best_nv = run_classification_dataset1_and_vowel(Xv, yv, dataset_name=\"vowel\",\n",
    "                                                                    mixtures=[1,2,4,8,16,32])\n",
    "        print(f\"Vowel dataset done. Best mixtures: {best_nv}\")\n",
    "    except Exception as e:\n",
    "        print(\"Vowel step skipped due to error:\", e)\n",
    "\n",
    "    # 3) Dataset 2(b) scene images\n",
    "    try:\n",
    "        print(\"Running scene image experiments (color hist & BoVW)...\")\n",
    "        scene_results = run_scene_classification(SCENE_DIR, mixtures=[1,2,4,8,16])\n",
    "        print(\"Scene experiments done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Scene experiments skipped due to error:\", e)\n",
    "\n",
    "    # 4) Dataset 2(c) cell segmentation\n",
    "    try:\n",
    "        print(\"Running cell segmentation...\")\n",
    "        seg_out = run_cell_segmentation(CELLS_DIR, out_segment_dir=os.path.join(OUT_DIR, \"cells_seg\"), k_clust=3)\n",
    "        print(\"Cell segmentation done.\")\n",
    "    except Exception as e:\n",
    "        print(\"Cell segmentation skipped due to error:\", e)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
