{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86c40fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv\n",
    "import os\n",
    "\n",
    "# ---------- Gaussian PDF ----------\n",
    "def gaussian_pdf(x, mean, cov):\n",
    "    d = len(x)\n",
    "    diff = x - mean\n",
    "    num = np.exp(-0.5 * diff.T @ inv(cov) @ diff)\n",
    "    den = np.sqrt((2 * np.pi) ** d * det(cov))\n",
    "    return num / (den + 1e-12)\n",
    "\n",
    "# ---------- Simple K-Means ----------\n",
    "def kmeans(X, K, tol=1e-4):\n",
    "    N, D = X.shape\n",
    "    np.random.seed(0)\n",
    "    centroids = X[np.random.choice(N, K, replace=False)]\n",
    "    labels = np.zeros(N, dtype=int)\n",
    "    while True:\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        new_labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([X[new_labels == k].mean(axis=0) for k in range(K)])\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids, labels = new_centroids, new_labels\n",
    "    return centroids, new_labels\n",
    "\n",
    "# ---------- GMM EM ----------\n",
    "def gmm_em(X, K, tol=1e-4):\n",
    "    N, D = X.shape\n",
    "    means, labels = kmeans(X, K)\n",
    "    covs = np.zeros((K, D, D))\n",
    "    weights = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        Xk = X[labels == k]\n",
    "        if len(Xk) == 0:\n",
    "            covs[k] = np.eye(D)\n",
    "        else:\n",
    "            covs[k] = np.cov(Xk, rowvar=False) + 1e-6 * np.eye(D)\n",
    "        weights[k] = len(Xk) / N\n",
    "\n",
    "    prev_log_likelihood = -np.inf\n",
    "    while True:\n",
    "        # E-step\n",
    "        resp = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            resp[:, k] = weights[k] * np.array([gaussian_pdf(x, means[k], covs[k]) for x in X])\n",
    "        resp_sum = resp.sum(axis=1, keepdims=True)\n",
    "        resp /= resp_sum\n",
    "\n",
    "        # M-step\n",
    "        Nk = resp.sum(axis=0)\n",
    "        for k in range(K):\n",
    "            means[k] = (resp[:, k][:, None] * X).sum(axis=0) / Nk[k]\n",
    "            diff = X - means[k]\n",
    "            covs[k] = (resp[:, k][:, None, None] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk[k]\n",
    "            covs[k] += 1e-6 * np.eye(D)\n",
    "            weights[k] = Nk[k] / N\n",
    "\n",
    "        # Convergence\n",
    "        log_likelihood = np.sum(np.log(resp_sum))\n",
    "        if abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "    return means, covs, weights\n",
    "\n",
    "\n",
    "# ---------- Naive Bayes + GMM ----------\n",
    "class NaiveBayesGMM:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.models = {}\n",
    "        self.class_priors = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes = np.unique(y)\n",
    "        for cls in classes:\n",
    "            Xc = X[y == cls]\n",
    "            means, covs, weights = gmm_em(Xc, self.n_components)\n",
    "            self.models[cls] = (means, covs, weights)\n",
    "            self.class_priors[cls] = len(Xc) / len(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for cls, (means, covs, weights) in self.models.items():\n",
    "                p_x_given_cls = np.sum([\n",
    "                    weights[k] * gaussian_pdf(x, means[k], covs[k])\n",
    "                    for k in range(self.n_components)\n",
    "                ])\n",
    "                p_cls = self.class_priors[cls]\n",
    "                class_probs[cls] = p_x_given_cls * p_cls\n",
    "            preds.append(max(class_probs, key=class_probs.get))\n",
    "        return np.array(preds)\n",
    "\n",
    "    # ---------- Save Model ----------\n",
    "    def save(self, folder_path=\"gmm_model\"):\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        np.save(os.path.join(folder_path, \"class_priors.npy\"), self.class_priors)\n",
    "        np.save(os.path.join(folder_path, \"n_components.npy\"), self.n_components)\n",
    "        # Save each class model separately\n",
    "        for cls, (means, covs, weights) in self.models.items():\n",
    "            np.save(os.path.join(folder_path, f\"class_{cls}_means.npy\"), means)\n",
    "            np.save(os.path.join(folder_path, f\"class_{cls}_covs.npy\"), covs)\n",
    "            np.save(os.path.join(folder_path, f\"class_{cls}_weights.npy\"), weights)\n",
    "\n",
    "    # ---------- Load Model ----------\n",
    "    def load(self, folder_path=\"gmm_model\"):\n",
    "        self.n_components = int(np.load(os.path.join(folder_path, \"n_components.npy\")))\n",
    "        self.class_priors = np.load(os.path.join(folder_path, \"class_priors.npy\"), allow_pickle=True).item()\n",
    "        self.models = {}\n",
    "        for cls in self.class_priors.keys():\n",
    "            means = np.load(os.path.join(folder_path, f\"class_{cls}_means.npy\"))\n",
    "            covs = np.load(os.path.join(folder_path, f\"class_{cls}_covs.npy\"))\n",
    "            weights = np.load(os.path.join(folder_path, f\"class_{cls}_weights.npy\"))\n",
    "            self.models[cls] = (means, covs, weights)\n",
    "\n",
    "def load_train_test_datasets(base_folder):\n",
    "    def load_folder(folder):\n",
    "        data = []\n",
    "        labels = []\n",
    "        class_map = {}  # Maps class name to label (e.g. 'class1' → 0)\n",
    "\n",
    "        files = [f for f in os.listdir(folder) if f.endswith(\".txt\")]\n",
    "        files.sort()  # Optional: for consistent ordering\n",
    "\n",
    "        for file in files:\n",
    "            path = os.path.join(folder, file)\n",
    "            parts = file.split(\"_\")\n",
    "            class_name = parts[0]  # 'class1' from 'class1_train.txt'\n",
    "\n",
    "            # Assign numeric label to class name\n",
    "            if class_name not in class_map:\n",
    "                class_map[class_name] = len(class_map)  # auto-labeling\n",
    "\n",
    "            label = class_map[class_name]\n",
    "\n",
    "            points = np.loadtxt(path, delimiter=' ')\n",
    "            data.append(points)\n",
    "            labels.append(np.full(len(points), label))\n",
    "\n",
    "        return np.vstack(data), np.hstack(labels), class_map\n",
    "\n",
    "    train_folder = os.path.join(base_folder, \"train\")\n",
    "    test_folder = os.path.join(base_folder, \"test\")\n",
    "\n",
    "    X_train, y_train, class_map = load_folder(train_folder)\n",
    "    X_test, y_test, _ = load_folder(test_folder)  # test uses same class_map, but we ignore it\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, class_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ced3aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, class_map = load_train_test_datasets(r'../../Dataset/Group04/NLS_Group04/')\n",
    "\n",
    "nb_gmm = NaiveBayesGMM(n_components=2)\n",
    "nb_gmm.fit(X_train, y_train)\n",
    "nb_gmm.save(\"Dataset\")\n",
    "\n",
    "print(\"✅ Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "566d4b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "loaded_model = NaiveBayesGMM()\n",
    "loaded_model.load(\"my_gmm_model\")\n",
    "\n",
    "y_pred = loaded_model.predict(X_train)\n",
    "acc = np.mean(y_pred == y_train)\n",
    "print(f\"Reloaded model accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118d45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import det, inv, slogdet\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Gaussian PDF (multivariate)\n",
    "# -----------------------------\n",
    "def gaussian_pdf(x, mean, cov):\n",
    "    \"\"\"Return multivariate Gaussian density value at x.\"\"\"\n",
    "    d = mean.shape[0]\n",
    "    diff = x - mean\n",
    "    # use slogdet for numeric stability\n",
    "    sign, logdet = slogdet(cov)\n",
    "    if sign <= 0:\n",
    "        cov = cov + 1e-6 * np.eye(d)\n",
    "        _, logdet = slogdet(cov)\n",
    "    invc = inv(cov)\n",
    "    exponent = -0.5 * diff.T @ invc @ diff\n",
    "    norm = -0.5 * (d * np.log(2*np.pi) + logdet)\n",
    "    return np.exp(norm + exponent)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Simple K-Means (for init)\n",
    "# -----------------------------\n",
    "def kmeans(X, K, tol=1e-4, max_iters=500):\n",
    "    N, D = X.shape\n",
    "    rng = np.random.default_rng(0)\n",
    "    centroids = X[rng.choice(N, K, replace=False)]\n",
    "    labels = np.zeros(N, dtype=int)\n",
    "    for it in range(max_iters):\n",
    "        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n",
    "        new_labels = np.argmin(dists, axis=1)\n",
    "        new_centroids = np.array([X[new_labels == k].mean(axis=0) if np.any(new_labels==k) else centroids[k] \n",
    "                                  for k in range(K)])\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            labels = new_labels\n",
    "            centroids = new_centroids\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "        labels = new_labels\n",
    "    return centroids, labels\n",
    "\n",
    "# -----------------------------\n",
    "# 3) GMM EM until convergence (returns params + logliks)\n",
    "# -----------------------------\n",
    "def gmm_em(X, K, tol=1e-4, safeguard_max_iters=1000):\n",
    "    \"\"\"\n",
    "    Fit GMM to X with K components.\n",
    "    Returns: means (K,D), covs (K,D,D), weights (K,), loglik_history (list)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    means, init_labels = kmeans(X, K)\n",
    "    covs = np.zeros((K, D, D))\n",
    "    weights = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        Xk = X[init_labels == k]\n",
    "        if len(Xk) < 2:\n",
    "            covs[k] = np.cov(X.T) + 1e-6*np.eye(D)\n",
    "        else:\n",
    "            covs[k] = np.cov(Xk, rowvar=False) + 1e-6*np.eye(D)\n",
    "        weights[k] = max(len(Xk) / N, 1e-6)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    loglik_history = []\n",
    "    prev_ll = -np.inf\n",
    "    it = 0\n",
    "    while True:\n",
    "        # E-step: responsibilities\n",
    "        resp = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            # vectorize gaussian evaluations\n",
    "            resp[:, k] = weights[k] * np.array([gaussian_pdf(x, means[k], covs[k]) for x in X])\n",
    "        row_sums = resp.sum(axis=1, keepdims=True)\n",
    "        # avoid division by zero\n",
    "        row_sums[row_sums == 0] = 1e-12\n",
    "        resp = resp / row_sums\n",
    "\n",
    "        # compute log-likelihood (stable)\n",
    "        ll = np.sum(np.log(row_sums + 1e-12))\n",
    "        loglik_history.append(ll)\n",
    "\n",
    "        # check convergence\n",
    "        if it > 0 and abs(ll - prev_ll) < tol:\n",
    "            break\n",
    "        prev_ll = ll\n",
    "\n",
    "        # M-step\n",
    "        Nk = resp.sum(axis=0)  # effective counts\n",
    "        for k in range(K):\n",
    "            if Nk[k] < 1e-8:\n",
    "                # reinitialize dead component\n",
    "                means[k] = X[np.random.randint(0, N)]\n",
    "                covs[k] = np.cov(X.T) + 1e-6*np.eye(D)\n",
    "                weights[k] = 1.0 / N\n",
    "                continue\n",
    "            means[k] = (resp[:, k][:, None] * X).sum(axis=0) / Nk[k]\n",
    "            diff = X - means[k]\n",
    "            covs[k] = (resp[:, k][:, None, None] * np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk[k]\n",
    "            covs[k] += 1e-6 * np.eye(D)\n",
    "            weights[k] = Nk[k] / N\n",
    "        weights /= weights.sum()\n",
    "\n",
    "        it += 1\n",
    "        if it >= safeguard_max_iters:\n",
    "            # safety: stop after many iterations\n",
    "            break\n",
    "\n",
    "    return means, covs, weights, loglik_history\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Naive Bayes GMM (multi-class)\n",
    "# -----------------------------\n",
    "class NaiveBayesGMM:\n",
    "    def __init__(self, n_components=2):\n",
    "        self.n_components = n_components\n",
    "        self.models = {}      # cls -> (means, covs, weights)\n",
    "        self.priors = {}      # cls -> prior\n",
    "        self.logliks = {}     # cls -> loglik_history (training)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        N = len(y)\n",
    "        for cls in classes:\n",
    "            Xc = X[y == cls]\n",
    "            means, covs, weights, loglik_hist = gmm_em(Xc, self.n_components)\n",
    "            self.models[cls] = (means, covs, weights)\n",
    "            self.priors[cls] = len(Xc) / N\n",
    "            self.logliks[cls] = loglik_hist\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # returns array (N, n_classes) of unnormalized posteriors\n",
    "        classes = list(self.models.keys())\n",
    "        N = X.shape[0]\n",
    "        post = np.zeros((N, len(classes)))\n",
    "        for i, cls in enumerate(classes):\n",
    "            means, covs, weights = self.models[cls]\n",
    "            K = means.shape[0]\n",
    "            # compute p(x|cls)\n",
    "            px = np.zeros(N)\n",
    "            for k in range(K):\n",
    "                px += weights[k] * np.array([gaussian_pdf(x, means[k], covs[k]) for x in X])\n",
    "            post[:, i] = px * self.priors[cls]\n",
    "        # normalize to get probs\n",
    "        denom = post.sum(axis=1, keepdims=True)\n",
    "        denom[denom==0] = 1e-12\n",
    "        probs = post / denom\n",
    "        return probs, list(self.models.keys())\n",
    "\n",
    "    def predict(self, X):\n",
    "        probs, classes = self.predict_proba(X)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        return np.array([classes[i] for i in idx])\n",
    "\n",
    "    # Save/load\n",
    "    def save(self, folder=\"gmm_saved\"):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        np.save(os.path.join(folder, \"n_components.npy\"), np.array([self.n_components]))\n",
    "        np.save(os.path.join(folder, \"classes.npy\"), np.array(list(self.models.keys()), dtype=object))\n",
    "        np.save(os.path.join(folder, \"priors.npy\"), self.priors)\n",
    "        for cls, (means, covs, weights) in self.models.items():\n",
    "            np.save(os.path.join(folder, f\"class_{cls}_means.npy\"), means)\n",
    "            np.save(os.path.join(folder, f\"class_{cls}_covs.npy\"), covs)\n",
    "            np.save(os.path.join(folder, f\"class_{cls}_weights.npy\"), weights)\n",
    "        # save logliks\n",
    "        np.save(os.path.join(folder, \"logliks.npy\"), self.logliks)\n",
    "\n",
    "    def load(self, folder=\"gmm_saved\"):\n",
    "        self.n_components = int(np.load(os.path.join(folder, \"n_components.npy\"))[0])\n",
    "        classes = np.load(os.path.join(folder, \"classes.npy\"), allow_pickle=True)\n",
    "        self.priors = np.load(os.path.join(folder, \"priors.npy\"), allow_pickle=True).item()\n",
    "        self.models = {}\n",
    "        for cls in classes:\n",
    "            means = np.load(os.path.join(folder, f\"class_{cls}_means.npy\"))\n",
    "            covs = np.load(os.path.join(folder, f\"class_{cls}_covs.npy\"))\n",
    "            weights = np.load(os.path.join(folder, f\"class_{cls}_weights.npy\"))\n",
    "            self.models[cls] = (means, covs, weights)\n",
    "        self.logliks = np.load(os.path.join(folder, \"logliks.npy\"), allow_pickle=True).item()\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Metrics: precision, recall, F1, accuracy per-class\n",
    "# -----------------------------\n",
    "def confusion_matrix(y_true, y_pred, classes=None):\n",
    "    if classes is None:\n",
    "        classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "    C = np.zeros((len(classes), len(classes)), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        C[class_to_idx[t], class_to_idx[p]] += 1\n",
    "    return C, classes\n",
    "\n",
    "def classification_report(y_true, y_pred):\n",
    "    C, classes = confusion_matrix(y_true, y_pred)\n",
    "    TP = np.diag(C)\n",
    "    FP = C.sum(axis=0) - TP\n",
    "    FN = C.sum(axis=1) - TP\n",
    "    TN = C.sum() - (TP + FP + FN)\n",
    "\n",
    "    precision = np.zeros(len(classes))\n",
    "    recall = np.zeros(len(classes))\n",
    "    f1 = np.zeros(len(classes))\n",
    "    support = C.sum(axis=1)\n",
    "\n",
    "    for i in range(len(classes)):\n",
    "        precision[i] = TP[i] / (TP[i] + FP[i]) if (TP[i] + FP[i]) > 0 else 0.0\n",
    "        recall[i] = TP[i] / (TP[i] + FN[i]) if (TP[i] + FN[i]) > 0 else 0.0\n",
    "        f1[i] = (2 * precision[i] * recall[i] / (precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0.0\n",
    "\n",
    "    accuracy = TP.sum() / C.sum() if C.sum() > 0 else 0.0\n",
    "\n",
    "    report = {\n",
    "        'classes': classes,\n",
    "        'confusion_matrix': C,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_per_class': precision,\n",
    "        'recall_per_class': recall,\n",
    "        'f1_per_class': f1,\n",
    "        'mean_precision': precision.mean(),\n",
    "        'mean_recall': recall.mean(),\n",
    "        'mean_f1': f1.mean(),\n",
    "        'support': support\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Plotting utilities\n",
    "# -----------------------------\n",
    "def plot_confusion_matrix(C, classes, cmap='Blues'):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(C, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(classes)):\n",
    "            plt.text(j, i, str(C[i,j]), ha='center', va='center', color='black')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_density_contours(model: NaiveBayesGMM, X, classes_to_plot=None, levels=[0.001,0.01,0.05,0.1,0.2,0.4]):\n",
    "    \"\"\"\n",
    "    Plot constant density contour for each class (mixture density p(x|class)), superposed with X.\n",
    "    Works for 2D data only.\n",
    "    \"\"\"\n",
    "    assert X.shape[1] == 2, \"Contour plot only for 2D data\"\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    if classes_to_plot is None:\n",
    "        classes_to_plot = list(model.models.keys())\n",
    "    plt.figure(figsize=(8,6))\n",
    "    colors = plt.cm.get_cmap('tab10')\n",
    "    for i, cls in enumerate(classes_to_plot):\n",
    "        means, covs, weights = model.models[cls]\n",
    "        # evaluate p(x|class)\n",
    "        px = np.zeros(len(grid))\n",
    "        for k in range(means.shape[0]):\n",
    "            px += weights[k] * np.array([gaussian_pdf(g, means[k], covs[k]) for g in grid])\n",
    "        px = px.reshape(xx.shape)\n",
    "        CS = plt.contour(xx, yy, px, levels=levels, alpha=0.8, cmap='coolwarm')\n",
    "        plt.clabel(CS, inline=1, fontsize=8)\n",
    "    plt.scatter(X[:,0], X[:,1], c='k', s=10, alpha=0.6)\n",
    "    plt.title(\"Constant density contours (p(x|class)) with training data\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_decision_regions(model: NaiveBayesGMM, X, y, resolution=200):\n",
    "    assert X.shape[1] == 2, \"Decision region plot only for 2D data\"\n",
    "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
    "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, resolution), np.linspace(y_min, y_max, resolution))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid)\n",
    "    classes = list(model.models.keys())\n",
    "    class_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "    Zi = np.array([class_to_idx[z] for z in Z]).reshape(xx.shape)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.contourf(xx, yy, Zi, alpha=0.3, cmap='tab10')\n",
    "    plt.scatter(X[:,0], X[:,1], c=[class_to_idx[v] for v in y], cmap='tab10', edgecolor='k')\n",
    "    plt.title(\"Decision regions with training data\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loglik_iterations(model: NaiveBayesGMM):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for cls, hist in model.logliks.items():\n",
    "        plt.plot(hist, label=f\"class {cls}\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Log-Likelihood\")\n",
    "    plt.title(\"Iterations vs Log-Likelihood (per-class GMM training)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Image segmentation using GMM clusters (Dataset-2(c) style)\n",
    "# -----------------------------\n",
    "def gmm_segment_image(model_params, img_array):\n",
    "    \"\"\"\n",
    "    model_params: tuple (means, covs, weights) for clustering (K clusters)\n",
    "    img_array: HxW or HxWx3 numpy array; we flatten and run GMM responsibilities\n",
    "    returns segmented image where each pixel replaced by cluster index (0..K-1)\n",
    "    \"\"\"\n",
    "    means, covs, weights = model_params\n",
    "    H, W = img_array.shape[:2]\n",
    "    if img_array.ndim == 3:\n",
    "        data = img_array.reshape(-1, 3).astype(float)\n",
    "    else:\n",
    "        data = img_array.reshape(-1, 1).astype(float)\n",
    "    N = data.shape[0]\n",
    "    K = means.shape[0]\n",
    "    resp = np.zeros((N, K))\n",
    "    for k in range(K):\n",
    "        resp[:, k] = weights[k] * np.array([gaussian_pdf(x, means[k], covs[k]) for x in data])\n",
    "    labels = np.argmax(resp, axis=1)\n",
    "    seg = labels.reshape(H, W)\n",
    "    return seg\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Utility: run experiments for list of K values and return metrics + models\n",
    "# -----------------------------\n",
    "def run_experiments(X_train, y_train, X_test, y_test, K_list, n_components_per_class=None):\n",
    "    \"\"\"\n",
    "    For each K in K_list (common K for all classes), trains a NaiveBayesGMM,\n",
    "    computes test metrics, and stores log-likelihood histories.\n",
    "    Returns: dict keyed by K containing model, report, and other info.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for K in K_list:\n",
    "        print(\"Training for K =\", K)\n",
    "        model = NaiveBayesGMM(n_components=K)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "        C, classes = confusion_matrix(y_test, y_pred)\n",
    "        results[K] = {\n",
    "            'model': model,\n",
    "            'report': report,\n",
    "            'confusion_matrix': C,\n",
    "            'classes': classes,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "        print(f\"Done K={K}: acc={report['accuracy']:.4f}, mean_f1={report['mean_f1']:.4f}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b3d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, class_map = load_train_test_datasets(r'../../Dataset/Group04/NLS_Group04/')\n",
    "run_experiments(X_train, y_train, X_test, y_test,[2,4,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e52e7ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# After training or from results dict:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m[best_K][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;66;03m# choose best_K based on validation or metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m report \u001b[38;5;241m=\u001b[39m results[best_K][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# After training or from results dict:\n",
    "best_model = results[best_K]['model']   # choose best_K based on validation or metrics\n",
    "report = results[best_K]['report']\n",
    "print(\"Accuracy:\", report['accuracy'])\n",
    "print(\"Classes:\", report['classes'])\n",
    "for i, cls in enumerate(report['classes']):\n",
    "    print(f\"Class {cls}: precision={report['precision_per_class'][i]:.3f}, recall={report['recall_per_class'][i]:.3f}, f1={report['f1_per_class'][i]:.3f}\")\n",
    "print(\"Mean precision:\", report['mean_precision'])\n",
    "print(\"Mean recall:\", report['mean_recall'])\n",
    "print(\"Mean F1:\", report['mean_f1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc22d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dataset1 ===\n",
      "Accuracy: 1.0000\n",
      "Class 0: Precision=1.000, Recall=1.000, F1=1.000\n",
      "Class 1: Precision=1.000, Recall=1.000, F1=1.000\n",
      "Class 2: Precision=1.000, Recall=1.000, F1=1.000\n",
      "Mean Precision=1.000, Mean Recall=1.000, Mean F1=1.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "# ==============================\n",
    "# 1. Basic Gaussian PDF\n",
    "# ==============================\n",
    "def gaussian_pdf(x, mean, cov):\n",
    "    d = len(mean)\n",
    "    x_m = x - mean\n",
    "    return np.exp(-0.5 * x_m @ np.linalg.inv(cov) @ x_m.T) / (\n",
    "        np.sqrt((2 * np.pi) ** d * np.linalg.det(cov)) + 1e-12\n",
    "    )\n",
    "\n",
    "# ==============================\n",
    "# 2. Simple K-Means (for GMM init)\n",
    "# ==============================\n",
    "def kmeans(X, K, max_iters=100, tol=1e-4):\n",
    "    N, D = X.shape\n",
    "    centroids = X[np.random.choice(N, K, replace=False)]\n",
    "    for _ in range(max_iters):\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) for k in range(K)])\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, labels\n",
    "\n",
    "# ==============================\n",
    "# 3. GMM using EM\n",
    "# ==============================\n",
    "def gmm_em(X, K, tol=1e-4):\n",
    "    N, D = X.shape\n",
    "    means, labels = kmeans(X, K)\n",
    "    covs = np.array([np.cov(X[labels == k].T) + 1e-6*np.eye(D) for k in range(K)])\n",
    "    weights = np.array([np.mean(labels == k) for k in range(K)])\n",
    "\n",
    "    log_likelihood_list = []\n",
    "    prev_log_likelihood = -np.inf\n",
    "\n",
    "    while True:\n",
    "        # ---------- E-Step ----------\n",
    "        resp = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            for n in range(N):\n",
    "                resp[n, k] = weights[k] * gaussian_pdf(X[n], means[k], covs[k])\n",
    "        resp /= resp.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # ---------- M-Step ----------\n",
    "        Nk = resp.sum(axis=0)\n",
    "        for k in range(K):\n",
    "            means[k] = (resp[:, k][:, None] * X).sum(axis=0) / Nk[k]\n",
    "            diff = X - means[k]\n",
    "            covs[k] = (resp[:, k][:, None, None] *\n",
    "                       np.einsum('ni,nj->nij', diff, diff)).sum(axis=0) / Nk[k]\n",
    "            covs[k] += 1e-6 * np.eye(D)\n",
    "            weights[k] = Nk[k] / N\n",
    "\n",
    "        # ---------- Log Likelihood ----------\n",
    "        log_likelihood = np.sum(np.log(\n",
    "            np.sum([weights[k] *\n",
    "                    np.array([gaussian_pdf(x, means[k], covs[k]) for x in X])\n",
    "                    for k in range(K)], axis=0)\n",
    "        ))\n",
    "        log_likelihood_list.append(log_likelihood)\n",
    "\n",
    "        if abs(log_likelihood - prev_log_likelihood) < tol:\n",
    "            break\n",
    "        prev_log_likelihood = log_likelihood\n",
    "\n",
    "    return means, covs, weights, log_likelihood_list\n",
    "\n",
    "# ==============================\n",
    "# 4. Train per-class GMM\n",
    "# ==============================\n",
    "def train_gmm_classifier(X_train, y_train, K=3):\n",
    "    classes = np.unique(y_train)\n",
    "    models = {}\n",
    "    for c in classes:\n",
    "        Xc = X_train[y_train == c]\n",
    "        means, covs, weights, ll_list = gmm_em(Xc, K)\n",
    "        models[c] = (means, covs, weights)\n",
    "    return models\n",
    "\n",
    "# ==============================\n",
    "# 5. Predict class labels\n",
    "# ==============================\n",
    "def predict_gmm_classifier(X, models):\n",
    "    N = X.shape[0]\n",
    "    classes = list(models.keys())\n",
    "    scores = np.zeros((N, len(classes)))\n",
    "    for i, c in enumerate(classes):\n",
    "        means, covs, weights = models[c]\n",
    "        for k in range(len(weights)):\n",
    "            scores[:, i] += weights[k] * np.array(\n",
    "                [gaussian_pdf(x, means[k], covs[k]) for x in X]\n",
    "            )\n",
    "    return np.argmax(scores, axis=1)\n",
    "\n",
    "# ==============================\n",
    "# 6. Evaluation & Visualization\n",
    "# ==============================\n",
    "def evaluate_and_plot(dataset_name, X_train, y_train, X_test, y_test,\n",
    "                      y_pred, models, log_likelihood_list, results_dir=\"results_gmm\"):\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # ---- Metrics ----\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average=None)\n",
    "    rec = recall_score(y_test, y_pred, average=None)\n",
    "    f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "    print(f\"\\n=== {dataset_name} ===\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    for i in range(len(prec)):\n",
    "        print(f\"Class {i}: Precision={prec[i]:.3f}, Recall={rec[i]:.3f}, F1={f1[i]:.3f}\")\n",
    "    print(f\"Mean Precision={prec.mean():.3f}, Mean Recall={rec.mean():.3f}, Mean F1={f1.mean():.3f}\")\n",
    "\n",
    "    # ---- Confusion Matrix ----\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {dataset_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"confusion_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ---- Log Likelihood ----\n",
    "    plt.figure()\n",
    "    plt.plot(log_likelihood_list)\n",
    "    plt.title(f\"Log Likelihood Convergence - {dataset_name}\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Log Likelihood\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(results_dir, f\"loglikelihood_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # ---- Decision Region (2D only) ----\n",
    "    if X_train.shape[1] == 2:\n",
    "        x_min, x_max = X_train[:, 0].min()-1, X_train[:, 0].max()+1\n",
    "        y_min, y_max = X_train[:, 1].min()-1, X_train[:, 1].max()+1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                             np.linspace(y_min, y_max, 200))\n",
    "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        pred_grid = predict_gmm_classifier(grid, models)\n",
    "        plt.contourf(xx, yy, pred_grid.reshape(xx.shape), alpha=0.4, cmap='rainbow')\n",
    "        plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, edgecolor='k')\n",
    "        plt.title(f\"Decision Regions - {dataset_name}\")\n",
    "        plt.savefig(os.path.join(results_dir, f\"decision_{dataset_name}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# ==============================\n",
    "# 7. Example Run (synthetic data)\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, class_map = load_train_test_datasets(r'../../Dataset/Group04/NLS_Group04/')\n",
    "    dataset_name = \"Dataset1\"\n",
    "    models = train_gmm_classifier(X_train, y_train, K=3)\n",
    "    y_pred = predict_gmm_classifier(X_test, models)\n",
    "\n",
    "    # Collect log-likelihoods from one class for plotting\n",
    "    _, _, _, ll_list = gmm_em(X_train[y_train == 0], K=3)\n",
    "\n",
    "    evaluate_and_plot(dataset_name, X_train, y_train, X_test, y_test,\n",
    "                      y_pred, models, ll_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded6af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_contours(X_train, y_train, models, dataset_name, results_dir=\"results_gmm\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Only 2D data can be visualized\n",
    "    if X_train.shape[1] != 2:\n",
    "        print(\"Density contour plots only supported for 2D data.\")\n",
    "        return\n",
    "\n",
    "    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                         np.linspace(y_min, y_max, 200))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    # Plot density for each class\n",
    "    for c, (means, covs, weights) in models.items():\n",
    "        z_total = np.zeros(grid.shape[0])\n",
    "        for k in range(len(weights)):\n",
    "            z_total += weights[k] * np.array([gaussian_pdf(p, means[k], covs[k]) for p in grid])\n",
    "        plt.contour(xx, yy, z_total.reshape(xx.shape), levels=5, alpha=0.7, cmap='cool', linestyles='--')\n",
    "\n",
    "    # Plot training points\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, edgecolor='k')\n",
    "    plt.title(f\"Constant Density Contours - {dataset_name}\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.savefig(os.path.join(results_dir, f\"contours_{dataset_name}.png\"))\n",
    "    plt.close()\n",
    "    print(f\"Saved constant density contours for {dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d844e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved constant density contours for Dataset1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train2a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m plot_density_contours(X_train, y_train, models, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# For Dataset-2(a) (2D)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m plot_density_contours(\u001b[43mX_train2a\u001b[49m, y_train2a, models2a, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset2a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train2a' is not defined"
     ]
    }
   ],
   "source": [
    "# For Dataset-1 (2D)\n",
    "plot_density_contours(X_train, y_train, models, \"Dataset1\")\n",
    "\n",
    "# # For Dataset-2(a) (2D)\n",
    "# plot_density_contours(X_train2a, y_train2a, models2a, \"Dataset2a\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
