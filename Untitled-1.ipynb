{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b5c417",
   "metadata": {},
   "source": [
    "# GMM-based Classification and Visualization on Dataset2a_Speech\n",
    "\n",
    "This notebook demonstrates Gaussian Mixture Model (GMM) based classification and visualization for the Dataset2a_Speech dataset. The workflow includes data loading, clustering initialization, GMM fitting, classification, metrics calculation, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd38ff",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Utility Functions\n",
    "\n",
    "Import essential libraries and define utility functions for saving/loading `.npy` and `.json` files, and for directory creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def save_npy(obj, path):\n",
    "    np.save(path, obj)\n",
    "\n",
    "def load_npy(path):\n",
    "    return np.load(path, allow_pickle=True)\n",
    "\n",
    "def save_json(obj, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a542f",
   "metadata": {},
   "source": [
    "## 2. Data Loading\n",
    "\n",
    "Load training and testing data for each class from text files using the `load_txt_data` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_data(base_path, class_names):\n",
    "    train, test = [], []\n",
    "    for cls in class_names:\n",
    "        train.append(np.loadtxt(os.path.join(base_path, 'train', f'{cls}_train.txt')))\n",
    "        test.append(np.loadtxt(os.path.join(base_path, 'test', f'{cls}_test.txt')))\n",
    "    return train, test\n",
    "\n",
    "# Specify dataset path and class names\n",
    "base_path = r'd:\\IITDH\\Sem1\\SPRL\\SPR_Assignment\\SPR\\Dataset\\Group04\\rd_group4'\n",
    "class_names = ['class1', 'class2', 'class3']\n",
    "\n",
    "train2a, test2a = load_txt_data(base_path, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ccc94",
   "metadata": {},
   "source": [
    "## 3. K-means Clustering Implementation\n",
    "\n",
    "Implement the `kmeans` function to initialize cluster centroids for GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3af764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(X, K, max_iter=100, tol=1e-4):\n",
    "    np.random.seed(42)\n",
    "    N, D = X.shape\n",
    "    centroids = X[np.random.choice(N, K, replace=False)]\n",
    "    for it in range(max_iter):\n",
    "        dists = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)\n",
    "        labels = np.argmin(dists, axis=1)\n",
    "        new_centroids = np.array([X[labels == k].mean(axis=0) if np.any(labels == k) else centroids[k] for k in range(K)])\n",
    "        if np.linalg.norm(new_centroids - centroids) < tol:\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12798ff5",
   "metadata": {},
   "source": [
    "## 4. Gaussian Mixture Model (GMM) via EM Algorithm\n",
    "\n",
    "Implement the `gmm_em` function for fitting GMMs to each class using the EM algorithm, including the `gaussian_pdf` helper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_pdf(X, mean, cov):\n",
    "    D = X.shape[1]\n",
    "    cov_det = np.linalg.det(cov)\n",
    "    cov_inv = np.linalg.inv(cov)\n",
    "    norm_const = 1.0 / (np.power((2*np.pi), D/2) * np.sqrt(cov_det + 1e-10))\n",
    "    diff = X - mean\n",
    "    exp_term = np.exp(-0.5 * np.sum(diff @ cov_inv * diff, axis=1))\n",
    "    return norm_const * exp_term\n",
    "\n",
    "def gmm_em(X, K, max_iter=100, tol=1e-4):\n",
    "    N, D = X.shape\n",
    "    means, labels = kmeans(X, K)\n",
    "    covs = np.array([np.cov(X[labels == k].T) + 1e-6*np.eye(D) if np.any(labels == k) else np.eye(D) for k in range(K)])\n",
    "    weights = np.array([np.mean(labels == k) for k in range(K)])\n",
    "    log_likelihoods = []\n",
    "    for it in range(max_iter):\n",
    "        resp = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            resp[:, k] = weights[k] * gaussian_pdf(X, means[k], covs[k])\n",
    "        resp_sum = resp.sum(axis=1, keepdims=True)\n",
    "        resp = resp / (resp_sum + 1e-10)\n",
    "        Nk = resp.sum(axis=0)\n",
    "        weights = Nk / N\n",
    "        means = np.array([np.sum(resp[:, k][:, None] * X, axis=0) / Nk[k] for k in range(K)])\n",
    "        covs = np.array([\n",
    "            ((resp[:, k][:, None] * (X - means[k])).T @ (X - means[k])) / Nk[k] + 1e-6*np.eye(D)\n",
    "            for k in range(K)\n",
    "        ])\n",
    "        ll = np.sum(np.log(resp_sum + 1e-10))\n",
    "        log_likelihoods.append(ll)\n",
    "        if it > 0 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < tol:\n",
    "            break\n",
    "    params = {'means': means, 'covs': covs, 'weights': weights}\n",
    "    return params, log_likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217280a9",
   "metadata": {},
   "source": [
    "## 5. Bayes Classifier Implementation\n",
    "\n",
    "Implement `bayes_classifier` to predict class labels for test data using fitted GMM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_classifier(X, gmm_params_list):\n",
    "    N = X.shape[0]\n",
    "    num_classes = len(gmm_params_list)\n",
    "    scores = np.zeros((N, num_classes))\n",
    "    for c, params in enumerate(gmm_params_list):\n",
    "        K = len(params['weights'])\n",
    "        prob = np.zeros((N, K))\n",
    "        for k in range(K):\n",
    "            prob[:, k] = params['weights'][k] * gaussian_pdf(X, params['means'][k], params['covs'][k])\n",
    "        scores[:, c] = prob.sum(axis=1)\n",
    "    preds = np.argmax(scores, axis=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4204b8",
   "metadata": {},
   "source": [
    "## 6. Metrics Calculation\n",
    "\n",
    "Implement `compute_metrics` to calculate accuracy, precision, recall, F1-score, and confusion matrix. Include `print_metrics` for formatted output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b543776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, num_classes):\n",
    "    cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        cm[t, p] += 1\n",
    "    acc = np.trace(cm) / np.sum(cm)\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1 = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = cm[:, i].sum() - tp\n",
    "        fn = cm[i, :].sum() - tp\n",
    "        precision[i] = tp / (tp + fp + 1e-10)\n",
    "        recall[i] = tp / (tp + fn + 1e-10)\n",
    "        f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i] + 1e-10)\n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'precision': precision.tolist(),\n",
    "        'mean_precision': np.mean(precision),\n",
    "        'recall': recall.tolist(),\n",
    "        'mean_recall': np.mean(recall),\n",
    "        'f1': f1.tolist(),\n",
    "        'mean_f1': np.mean(f1),\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(\"Precision per class:\", [\"{:.4f}\".format(p) for p in metrics['precision']])\n",
    "    print(\"Mean Precision:\", \"{:.4f}\".format(metrics['mean_precision']))\n",
    "    print(\"Recall per class:\", [\"{:.4f}\".format(r) for r in metrics['recall']])\n",
    "    print(\"Mean Recall:\", \"{:.4f}\".format(metrics['mean_recall']))\n",
    "    print(\"F1 per class:\", [\"{:.4f}\".format(f) for f in metrics['f1']])\n",
    "    print(\"Mean F1:\", \"{:.4f}\".format(metrics['mean_f1']))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(np.array(metrics['confusion_matrix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f6a4b",
   "metadata": {},
   "source": [
    "## 7. Experiment Runner for Dataset2a_Speech\n",
    "\n",
    "Use `run_experiment` to fit GMMs for each class, run classification, and save results for multiple values of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd1807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(dataset_name, train_data, test_data, mixture_counts, result_dir):\n",
    "    ensure_dir(result_dir)\n",
    "    num_classes = len(train_data)\n",
    "    for K in mixture_counts:\n",
    "        print(f'Running {dataset_name} with {K} mixtures...')\n",
    "        gmm_params_list = []\n",
    "        log_likelihoods_list = []\n",
    "        for i, cls_data in enumerate(train_data):\n",
    "            params, log_likelihoods = gmm_em(cls_data, K)\n",
    "            gmm_params_list.append(params)\n",
    "            log_likelihoods_list.append(log_likelihoods)\n",
    "            save_npy(params, f'{result_dir}/gmm_params_class{i}_K{K}.npy')\n",
    "            save_npy(log_likelihoods, f'{result_dir}/gmm_loglik_class{i}_K{K}.npy')\n",
    "        y_true = []\n",
    "        X_test = []\n",
    "        for i, cls_test in enumerate(test_data):\n",
    "            y_true.extend([i]*len(cls_test))\n",
    "            X_test.append(cls_test)\n",
    "        X_test = np.vstack(X_test)\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = bayes_classifier(X_test, gmm_params_list)\n",
    "        save_npy(y_true, f'{result_dir}/y_true_K{K}.npy')\n",
    "        save_npy(y_pred, f'{result_dir}/y_pred_K{K}.npy')\n",
    "        metrics = compute_metrics(y_true, y_pred, num_classes)\n",
    "        save_json(metrics, f'{result_dir}/metrics_K{K}.json')\n",
    "        print(f'Accuracy: {metrics[\"accuracy\"]:.4f}, Mean F1: {metrics[\"mean_f1\"]:.4f}')\n",
    "\n",
    "# Run experiment for Dataset2a_Speech\n",
    "result_dir = 'results/Dataset2a_Speech'\n",
    "run_experiment('Dataset2a_Speech', train2a, test2a, [1,2,4], result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222abd2",
   "metadata": {},
   "source": [
    "## 8. Load and Visualize GMM Results\n",
    "\n",
    "Load saved GMM parameters, log-likelihoods, and metrics. Visualize results using contour plots, decision regions, log-likelihood curves, and elliptical contours."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
